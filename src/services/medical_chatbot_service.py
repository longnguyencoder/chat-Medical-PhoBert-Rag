import os
import json
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
from openai import OpenAI
from typing import Dict, List, Optional, Any, Tuple
import chromadb
import numpy as np
import sys
import logging
import re
from collections import defaultdict

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Add src to path to import phobert_embedding
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(src_dir)

from src.nlp_model.phobert_embedding import PhoBERTEmbeddingFunction
from src.services.bm25_search import BM25SearchEngine, create_searchable_text
from src.services.hospital_finder_service import hospital_finder_service  # Hospital Finder
from src.services.tool_calling_functions import AVAILABLE_TOOLS, execute_tool_call  # Tool Calling

# Import Cross-Encoder for reranking
try:
    from sentence_transformers import CrossEncoder
    RERANKER = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    RERANKING_ENABLED = True
    logger.info("‚úì Cross-Encoder loaded for reranking")
except ImportError:
    RERANKING_ENABLED = False
    logger.warning("‚ö† sentence-transformers not installed. Reranking disabled.")

# Initialize BM25 search engine
BM25_ENGINE = BM25SearchEngine()
BM25_ENABLED = False  # Will be set to True after indexing

# Initialize OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Initialize ChromaDB client
workspace_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
chroma_client = chromadb.PersistentClient(path=os.path.join(workspace_root, 'src', 'nlp_model', 'data', 'chroma_db'))

# Initialize PhoBERT embedding function
phobert_ef = PhoBERTEmbeddingFunction()

# Medical keywords for enhanced relevance scoring
MEDICAL_KEYWORDS = {
    'symptoms': ['tri·ªáu ch·ª©ng', 'd·∫•u hi·ªáu', 'bi·ªÉu hi·ªán', 's·ªët', 'ho', 'ƒëau', 'ng·ª©a', 'm·ªát', 'bu·ªìn n√¥n'],
    'treatment': ['ƒëi·ªÅu tr·ªã', 'ch·ªØa', 'thu·ªëc', 'u·ªëng', 'd√πng', 'kh√°m', 'b√°c sƒ©'],
    'prevention': ['ph√≤ng ng·ª´a', 'tr√°nh', 'v·ªá sinh', 'v·∫Øc-xin', 'ti√™m ch·ªßng'],
    'diagnosis': ['ch·∫©n ƒëo√°n', 'x√©t nghi·ªám', 'ki·ªÉm tra', 'kh√°m']
}

# Confidence threshold for search results
CONFIDENCE_THRESHOLD = 0.15

# Hybrid search weights (BM25 + Vector)
HYBRID_BM25_WEIGHT = 0.3  # 30% BM25 keyword matching
HYBRID_VECTOR_WEIGHT = 0.7  # 70% semantic vector search

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# RAG OPTIMIZATION: Query Expansion & Reranking
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def expand_query(question: str) -> List[str]:
    """
    Expand user query into multiple similar queries using GPT.
    This helps find more relevant results.
    
    Args:
        question: Original user question
        
    Returns:
        List of expanded queries (including original)
    """
    try:
        prompt = f"""B·∫°n l√† chuy√™n gia y t·∫ø. H√£y t·∫°o 2 c√¢u h·ªèi T∆Ø∆†NG T·ª∞ (kh√¥ng gi·ªëng h·ªát) v·ªõi c√¢u h·ªèi g·ªëc.

C√¢u h·ªèi g·ªëc: "{question}"

Y√™u c·∫ßu:
- Gi·ªØ nguy√™n √Ω nghƒ©a y t·∫ø
- D√πng t·ª´ ƒë·ªìng nghƒ©a ho·∫∑c c√°ch di·ªÖn ƒë·∫°t kh√°c
- M·ªói c√¢u tr√™n 1 d√≤ng
- KH√îNG gi·∫£i th√≠ch, CH·ªà tr·∫£ v·ªÅ 2 c√¢u h·ªèi

V√≠ d·ª•:
C√¢u g·ªëc: "S·ªët cao l√† bao nhi√™u ƒë·ªô?"
C√¢u 1: Nhi·ªát ƒë·ªô c∆° th·ªÉ bao nhi√™u ƒë∆∞·ª£c coi l√† s·ªët cao?
C√¢u 2: S·ªët tr√™n bao nhi√™u ƒë·ªô C l√† nguy hi·ªÉm?
"""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=150
        )
        
        expanded_text = response.choices[0].message.content.strip()
        expanded_queries = [q.strip() for q in expanded_text.split('\n') if q.strip()]
        
        # Always include original question first
        all_queries = [question] + expanded_queries[:2]
        logger.info(f"Query expansion: {question} ‚Üí {len(all_queries)} queries")
        return all_queries
        
    except Exception as e:
        logger.warning(f"Query expansion failed: {e}. Using original query only.")
        return [question]

def rewrite_query_with_context(question: str, conversation_id: int) -> str:
    """
    Rewrite user question to be self-contained based on conversation history.
    Example: "N√≥ c√≥ nguy hi·ªÉm kh√¥ng?" -> "B·ªánh s·ªët xu·∫•t huy·∫øt c√≥ nguy hi·ªÉm kh√¥ng?"
    """
    try:
        from src.models.message import Message
        
        # Get last 2 messages (user + bot pair)
        recent_messages = Message.query.filter_by(
            conversation_id=conversation_id
        ).order_by(Message.sent_at.desc()).limit(2).all()
        
        if not recent_messages:
            return question
            
        recent_messages.reverse()
        history_text = "\n".join([f"{'User' if m.sender=='user' else 'Bot'}: {m.message_text}" for m in recent_messages])
        
        prompt = f"""H√£y vi·∫øt l·∫°i c√¢u h·ªèi cu·ªëi c√πng c·ªßa User ƒë·ªÉ n√≥ ƒê·∫¶Y ƒê·ª¶ √ù NGHƒ®A, d·ª±a v√†o ng·ªØ c·∫£nh tr∆∞·ªõc ƒë√≥.

L·ªãch s·ª≠:
{history_text}

C√¢u h·ªèi hi·ªán t·∫°i: "{question}"

Y√™u c·∫ßu:
- N·∫øu c√¢u h·ªèi ƒë√£ r√µ r√†ng, gi·ªØ nguy√™n.
- N·∫øu c√¢u h·ªèi thi·∫øu ch·ªß ng·ªØ/ng·ªØ c·∫£nh (v√≠ d·ª•: "N√≥ l√† g√¨?", "U·ªëng thu·ªëc g√¨?"), h√£y th√™m t√™n b·ªánh/v·∫•n ƒë·ªÅ t·ª´ l·ªãch s·ª≠ v√†o.
- CH·ªà tr·∫£ v·ªÅ c√¢u h·ªèi ƒë√£ vi·∫øt l·∫°i (ho·∫∑c c√¢u g·ªëc). KH√îNG gi·∫£i th√≠ch.

C√¢u h·ªèi vi·∫øt l·∫°i:"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3, # Low temp for precision
            max_tokens=100
        )
        
        rewritten = response.choices[0].message.content.strip()
        logger.info(f"üîÑ Rewrote query: '{question}' -> '{rewritten}'")
        return rewritten
        
    except Exception as e:
        logger.warning(f"Query rewrite failed: {e}")
        return question

def rerank_results(question: str, results: List[Dict]) -> List[Dict]:
    """
    Rerank search results using Cross-Encoder for better accuracy.
    
    Args:
        question: User's question
        results: List of search results from PhoBERT
        
    Returns:
        Reranked results sorted by Cross-Encoder scores
    """
    if not RERANKING_ENABLED or not results:
        return results
    
    try:
        # Prepare pairs for Cross-Encoder
        pairs = []
        for result in results:
            # Combine all relevant text from metadata
            doc_text = f"{result['metadata'].get('disease_name', '')} "
            doc_text += f"{result['metadata'].get('symptoms', '')} "
            doc_text += f"{result['metadata'].get('treatment', '')}"
            pairs.append([question, doc_text])
        
        # Get Cross-Encoder scores
        ce_scores = RERANKER.predict(pairs)
        
        # Add Cross-Encoder scores to results
        for i, result in enumerate(results):
            result['ce_score'] = float(ce_scores[i])
            # Combine with original relevance score (70% CE, 30% original)
            result['final_score'] = 0.7 * ce_scores[i] + 0.3 * result.get('relevance_score', 0)
        
        # Sort by final score
        reranked = sorted(results, key=lambda x: x['final_score'], reverse=True)
        
        logger.info(f"Reranked {len(results)} results. Top score: {reranked[0]['final_score']:.3f}")
        return reranked
        
    except Exception as e:
        logger.error(f"Reranking failed: {e}. Using original order.")
        return results

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CONVERSATION SUMMARY
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def generate_conversation_summary(conversation_id: int) -> Optional[str]:
    """
    Generate a concise summary of the conversation using GPT.
    
    Args:
        conversation_id: ID of the conversation to summarize
        
    Returns:
        Concise summary string or None if failed
    """
    try:
        from src.models.message import Message
        
        # Get all messages in conversation
        messages = Message.query.filter_by(
            conversation_id=conversation_id
        ).order_by(Message.sent_at).all()
        
        if not messages or len(messages) < 3:
            return None  # Too few messages to summarize
        
        # Format conversation history
        conversation_text = []
        for msg in messages:
            sender = "Ng∆∞·ªùi d√πng" if msg.sender == 'user' else "B√°c sƒ© AI"
            conversation_text.append(f"{sender}: {msg.message_text}")
        
        full_conversation = "\n".join(conversation_text)
        
        # Generate summary with GPT
        prompt = f"""B·∫°n l√† tr·ª£ l√Ω y t·∫ø. H√£y t√≥m t·∫Øt cu·ªôc h·ªôi tho·∫°i sau th√†nh 3-5 d√≤ng NG·∫ÆN G·ªåN.

Cu·ªôc h·ªôi tho·∫°i:
{full_conversation}

Y√äU C·∫¶U T√ìM T·∫ÆT:
- Ch·ªâ ghi c√°c th√¥ng tin Y T·∫æ quan tr·ªçng
- Format: Bullet points (‚Ä¢)
- Bao g·ªìm: Tri·ªáu ch·ª©ng, thu·ªëc ƒë√£ d√πng, t√¨nh tr·∫°ng hi·ªán t·∫°i
- KH√îNG gi·∫£i th√≠ch, CH·ªà li·ªát k√™ th√¥ng tin

V√ç D·ª§ T√ìM T·∫ÆT T·ªêT:
‚Ä¢ Tri·ªáu ch·ª©ng: S·ªët 38¬∞C, ƒëau ƒë·∫ßu, ho khan
‚Ä¢ ƒê√£ d√πng: Paracetamol 3 ng√†y
‚Ä¢ T√¨nh tr·∫°ng: Ch∆∞a ƒë·ª°
‚Ä¢ Khuy·∫øn c√°o: C·∫ßn ƒëi kh√°m n·∫øu kh√¥ng c·∫£i thi·ªán

H√£y t√≥m t·∫Øt:"""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,  # Low temperature for consistent summaries
            max_tokens=200
        )
        
        summary = response.choices[0].message.content.strip()
        logger.info(f"‚úì Generated summary for conversation {conversation_id}")
        return summary
        
    except Exception as e:
        logger.error(f"Failed to generate summary: {e}")
        return None


def get_or_create_collection():
    """Get existing collection or create new one if not exists"""
    try:
        collection = chroma_client.get_collection(
            name="medical_collection",
            embedding_function=phobert_ef
        )
        return collection
    except Exception as e:
        print(f"Collection not found, creating new one: {str(e)}")
        collection = chroma_client.create_collection(
            name="medical_collection",
            embedding_function=phobert_ef
        )
        return collection

def initialize_bm25_index():
    """
    Initialize BM25 index with all documents from ChromaDB.
    This should be called once at startup.
    """
    global BM25_ENABLED
    
    try:
        logger.info("Initializing BM25 index...")
        collection = get_or_create_collection()
        
        # Get all documents from ChromaDB
        all_docs = collection.get(
            include=["documents", "metadatas"]
        )
        
        if not all_docs['ids']:
            logger.warning("No documents found in ChromaDB. BM25 index not created.")
            return False
        
        # Create searchable texts from metadata
        searchable_texts = [
            create_searchable_text(metadata) 
            for metadata in all_docs['metadatas']
        ]
        
        # Index documents
        BM25_ENGINE.index_documents(
            documents=searchable_texts,
            document_ids=all_docs['ids'],
            metadatas=all_docs['metadatas']
        )
        
        BM25_ENABLED = True
        logger.info(f"‚úì BM25 index initialized with {len(all_docs['ids'])} documents")
        return True
        
    except Exception as e:
        logger.error(f"Failed to initialize BM25 index: {e}")
        BM25_ENABLED = False
        return False

def normalize_similarity(distance: float) -> float:
    """Convert L2 distance to a normalized similarity score"""
    if distance <= 0:
        return 1.0
    sim = 1 / (1 + (distance / 10))
    return float(sim)

def extract_keywords(text: str) -> List[str]:
    """Extract important keywords from text"""
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    words = text.split()
    stop_words = {'l√†', 'c·ªßa', 'v√†', 'c√≥', 'ƒë∆∞·ª£c', 'n√†y', 'ƒë√≥', 'c√°c', 'cho', 't·ª´', 'v·ªõi', 'm·ªôt', 'nh·ªØng'}
    keywords = [w for w in words if w not in stop_words and len(w) > 2]
    return keywords

def calculate_keyword_match_score(question: str, document: str, metadata: Dict) -> float:
    """Calculate keyword matching score between question and document"""
    question_keywords = set(extract_keywords(question))
    searchable_text = ' '.join([
        str(metadata.get('disease_name', '')),
        str(metadata.get('symptoms', '')),
        str(metadata.get('treatment', '')),
        str(metadata.get('prevention', '')),
        str(metadata.get('description', ''))
    ]).lower()
    doc_keywords = set(extract_keywords(searchable_text))
    if not question_keywords or not doc_keywords:
        return 0.0
    intersection = len(question_keywords & doc_keywords)
    union = len(question_keywords | doc_keywords)
    return intersection / union if union > 0 else 0.0

def calculate_medical_relevance_score(question: str, metadata: Dict) -> float:
    """Calculate relevance score based on medical domain knowledge"""
    question_lower = question.lower()
    score = 0.0
    for category, keywords in MEDICAL_KEYWORDS.items():
        if any(kw in question_lower for kw in keywords):
            field_value = str(metadata.get(category, '')).lower()
            if field_value and len(field_value) > 5:
                score += 0.15
    return min(score, 0.6)

def calculate_combined_score(
    distance: float,
    question: str,
    document: str,
    metadata: Dict
) -> Tuple[float, Dict[str, float]]:
    """Calculate combined relevance score using multiple signals"""
    semantic_score = normalize_similarity(distance)
    keyword_score = calculate_keyword_match_score(question, document, metadata)
    medical_score = calculate_medical_relevance_score(question, metadata)
    final_score = (
        0.5 * semantic_score +
        0.3 * keyword_score +
        0.2 * medical_score
    )
    score_breakdown = {
        'semantic': round(semantic_score, 3),
        'keyword': round(keyword_score, 3),
        'medical': round(medical_score, 3),
        'final': round(final_score, 3)
    }
    return final_score, score_breakdown

def extract_user_intent_and_features(question: str) -> Dict[str, Any]:
    """Extract user intent and medical features using OpenAI"""
    tools_schema = [
        {
            "type": "function",
            "function": {
                "name": "tim_kiem_thong_tin_y_te",
                "description": "Tr√≠ch xu·∫•t th√¥ng tin y t·∫ø t·ª´ c√¢u h·ªèi ng∆∞·ªùi d√πng",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "trieu_chung": {"type": "string", "description": "C√°c tri·ªáu ch·ª©ng b·ªánh"},
                        "ten_benh": {"type": "string", "description": "T√™n b·ªánh nghi ng·ªù"},
                        "thuoc": {"type": "string", "description": "T√™n thu·ªëc ho·∫∑c lo·∫°i thu·ªëc"},
                        "muc_dich": {"type": "string", "description": "M·ª•c ƒë√≠ch h·ªèi"}
                    },
                    "required": []
                }
            }
        }
    ]
    system_prompt = "B·∫°n l√† tr·ª£ l√Ω y t·∫ø AI. H√£y ph√¢n t√≠ch c√¢u h·ªèi v√† tr√≠ch xu·∫•t th√¥ng tin y t·∫ø quan tr·ªçng."
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            tools=tools_schema,
            tool_choice={"type": "function", "function": {"name": "tim_kiem_thong_tin_y_te"}}
        )
        tool_calls = response.choices[0].message.tool_calls
        if tool_calls:
            function_args = json.loads(tool_calls[0].function.arguments)
            return {
                "original_question": question,
                "intent": "tim_kiem_thong_tin_y_te",
                "confidence": 0.9,
                "extracted_features": function_args
            }
        return {
            "original_question": question,
            "intent": "general_question",
            "confidence": 0.5,
            "extracted_features": {}
        }
    except Exception as e:
        print(f"Error in extract_user_intent_and_features: {str(e)}")
        return {
            "original_question": question,
            "intent": "error",
            "confidence": 0.0,
            "extracted_features": {}
        }

def hybrid_search(
    question: str,
    n_results: int = 10
) -> List[Dict[str, Any]]:
    """
    Perform hybrid search combining BM25 (keyword) and Vector (semantic) search.
    
    Args:
        question: User's search query
        n_results: Number of results to return
        
    Returns:
        Combined and scored results from both search methods
    """
    results_dict = defaultdict(lambda: {'bm25_score': 0.0, 'vector_score': 0.0})
    
    # === BM25 KEYWORD SEARCH ===
    if BM25_ENABLED:
        try:
            bm25_results = BM25_ENGINE.search(question, top_k=n_results * 2)
            
            # Normalize BM25 scores to 0-1 range
            if bm25_results:
                max_bm25 = max(r['bm25_score'] for r in bm25_results)
                if max_bm25 > 0:
                    for result in bm25_results:
                        doc_id = result['id']
                        normalized_score = result['bm25_score'] / max_bm25
                        results_dict[doc_id]['bm25_score'] = normalized_score
                        results_dict[doc_id]['metadata'] = result['metadata']
                        results_dict[doc_id]['document'] = result['document']
                        results_dict[doc_id]['id'] = doc_id
            
            logger.info(f"BM25 found {len(bm25_results)} results")
        except Exception as e:
            logger.error(f"BM25 search failed: {e}")
    
    # === VECTOR SEMANTIC SEARCH ===
    try:
        collection = get_or_create_collection()
        query_vec = phobert_ef([question])[0]
        vector_results = collection.query(
            query_embeddings=[query_vec],
            n_results=n_results * 2,
            include=["metadatas", "documents", "distances"]
        )
        
        # Process vector results
        for i in range(len(vector_results['ids'][0])):
            doc_id = vector_results['ids'][0][i]
            distance = vector_results['distances'][0][i]
            
            # Normalize distance to similarity score (0-1)
            vector_score = normalize_similarity(distance)
            
            results_dict[doc_id]['vector_score'] = vector_score
            results_dict[doc_id]['metadata'] = vector_results['metadatas'][0][i]
            results_dict[doc_id]['document'] = vector_results['documents'][0][i]
            results_dict[doc_id]['id'] = doc_id
            results_dict[doc_id]['distance'] = distance
        
        logger.info(f"Vector search found {len(vector_results['ids'][0])} results")
    except Exception as e:
        logger.error(f"Vector search failed: {e}")
        return []
    
    # === COMBINE SCORES ===
    combined_results = []
    for doc_id, scores in results_dict.items():
        # Hybrid score: weighted combination
        hybrid_score = (
            HYBRID_BM25_WEIGHT * scores['bm25_score'] + 
            HYBRID_VECTOR_WEIGHT * scores['vector_score']
        )
        
        combined_results.append({
            'id': scores['id'],
            'metadata': scores['metadata'],
            'document': scores['document'],
            'bm25_score': scores['bm25_score'],
            'vector_score': scores['vector_score'],
            'hybrid_score': hybrid_score,
            'relevance_score': hybrid_score,  # For compatibility
            'distance': scores.get('distance', 0),
            'score_breakdown': {
                'bm25': round(scores['bm25_score'], 3),
                'vector': round(scores['vector_score'], 3),
                'hybrid': round(hybrid_score, 3)
            }
        })
    
    # Sort by hybrid score
    combined_results.sort(key=lambda x: x['hybrid_score'], reverse=True)
    
    logger.info(f"Hybrid search combined {len(combined_results)} unique results")
    if combined_results:
        top = combined_results[0]
        logger.info(f"Top result: BM25={top['bm25_score']:.3f}, Vector={top['vector_score']:.3f}, Hybrid={top['hybrid_score']:.3f}")
    
    return combined_results[:n_results]

def combined_search_with_filters(
    question: str,
    extracted_features: Dict[str, Any],
    n_results: int = 10
) -> Dict[str, Any]:
    """
    Perform hybrid search with query expansion and reranking.
    
    FEATURES:
    - Hybrid Search: BM25 (keyword) + Vector (semantic)
    - Query Expansion: Generate similar queries to find more results
    - Reranking: Use Cross-Encoder to re-score results for better accuracy
    """
    try:
        logger.info(f"üîç Hybrid search for: {question}")
        collection = get_or_create_collection()
        count = collection.count()
        if count == 0:
            logger.warning("No data in database")
            return {"success": False, "message": "No data in database", "results": []}
        
        # === QUERY EXPANSION ===
        expanded_queries = expand_query(question)
        logger.info(f"Expanded to {len(expanded_queries)} queries")
        
        # === HYBRID SEARCH (BM25 + Vector) ===
        all_results = {}  # Use dict to deduplicate by ID
        
        for query in expanded_queries:
            # Perform hybrid search for each expanded query
            hybrid_results = hybrid_search(query, n_results=n_results * 2)
            
            # Merge results (keep best score for each document)
            for result in hybrid_results:
                result_id = result['id']
                if result_id not in all_results or result['hybrid_score'] > all_results[result_id]['relevance_score']:
                    all_results[result_id] = {
                        'id': result_id,
                        'metadata': result['metadata'],
                        'document': result['document'],
                        'distance': result.get('distance', 0),
                        'relevance_score': result['hybrid_score'],
                        'score_breakdown': result['score_breakdown'],
                        'bm25_score': result['bm25_score'],
                        'vector_score': result['vector_score'],
                        'confidence': 'high' if result['hybrid_score'] > 0.7 else 'medium' if result['hybrid_score'] > 0.5 else 'low'
                    }
        
        # Convert to list and sort
        scored_results = list(all_results.values())
        scored_results.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        # Take top candidates for reranking
        top_candidates = scored_results[:n_results * 2]
        
        # === RERANKING ===
        reranked_results = rerank_results(question, top_candidates)
        
        # Filter and limit
        filtered_results = [
            r for r in reranked_results
            if r.get('relevance_score', 0) >= CONFIDENCE_THRESHOLD
        ][:n_results]
        
        logger.info(f"Found {len(filtered_results)} relevant results (from {len(scored_results)} total)")
        if filtered_results:
            top = filtered_results[0]
            logger.info(f"Top result: {top['metadata'].get('disease_name')} "
                       f"(score: {top.get('final_score', top.get('relevance_score')):.3f})")
        
        return {
            "success": True,
            "results": filtered_results,
            "total_found": len(filtered_results),
            "total_searched": len(scored_results),
            "query_expansion_used": len(expanded_queries) > 1,
            "reranking_used": RERANKING_ENABLED,
            "hybrid_search_used": BM25_ENABLED,
            "search_method": "Hybrid (BM25 + Vector)" if BM25_ENABLED else "Vector Only"
        }
    except Exception as e:
        logger.error(f"Error in search: {str(e)}", exc_info=True)
        return {"success": False, "message": str(e), "results": []}

def generate_natural_response(
    question: str,
    search_results: List[Dict],
    extracted_features: Dict[str, Any],
    conversation_id: Optional[int] = None,
    user_name: Optional[str] = None
) -> Dict[str, Any]:
    """
    Generate natural language response using enhanced prompts.
    
    NEW: Includes conversation context and user personalization.
    """
    try:
        logger.info(f"Generating response with GPT (User: {user_name})")
        
        # === CONVERSATION CONTEXT ===
        conversation_context = ""
        conversation_summary = ""
        
        if conversation_id:
            try:
                from src.models.message import Message
                from src.models.conversation import Conversation
                
                # Get conversation summary (if exists)
                conversation = Conversation.query.get(conversation_id)
                if conversation and conversation.summary:
                    conversation_summary = conversation.summary
                    logger.info("‚úì Loaded conversation summary")
                
                # Get last 5 messages (excluding current question)
                recent_messages = Message.query.filter_by(
                    conversation_id=conversation_id
                ).order_by(Message.sent_at.desc()).limit(5).all()
                
                if recent_messages:
                    # Reverse to chronological order
                    recent_messages.reverse()
                    context_parts = []
                    for msg in recent_messages:
                        sender_label = "Ng∆∞·ªùi d√πng" if msg.sender == 'user' else "B√°c sƒ© AI"
                        context_parts.append(f"{sender_label}: {msg.message_text}")
                    
                    conversation_context = "\n".join(context_parts)
                    logger.info(f"‚úì Loaded {len(recent_messages)} recent messages for context")
            except Exception as e:
                logger.warning(f"Could not load conversation context: {e}")
        
        # === HEALTH PROFILE CONTEXT (NEW!) ===
        # L·∫•y h·ªì s∆° s·ª©c kh·ªèe c·ªßa user ƒë·ªÉ c√° nh√¢n h√≥a c√¢u tr·∫£ l·ªùi
        health_profile_context = ""
        if user_name:  # N·∫øu c√≥ user_name th√¨ c√≥ th·ªÉ l·∫•y ƒë∆∞·ª£c user_id
            try:
                from src.models.user import User
                from src.services.health_profile_service import health_profile_service
                
                # T√¨m user_id t·ª´ user_name (ho·∫∑c c√≥ th·ªÉ truy·ªÅn tr·ª±c ti·∫øp user_id v√†o h√†m n√†y)
                # T·∫°m th·ªùi skip v√¨ c·∫ßn refactor ƒë·ªÉ truy·ªÅn user_id v√†o
                # TODO: Refactor ƒë·ªÉ truy·ªÅn user_id v√†o generate_natural_response
                pass
            except Exception as e:
                logger.warning(f"Could not load health profile: {e}")
        
        # WORKAROUND: L·∫•y user_id t·ª´ conversation
        if conversation_id and not health_profile_context:
            try:
                from src.models.conversation import Conversation
                from src.services.health_profile_service import health_profile_service
                
                conversation = Conversation.query.get(conversation_id)
                if conversation:
                    user_id = conversation.user_id
                    profile_text = health_profile_service.format_profile_for_chatbot(user_id)
                    if profile_text:
                        health_profile_context = f"""
„ÄêH·ªí S∆† S·ª®C KH·ªéE C√Å NH√ÇN„Äë
{profile_text}

‚ö†Ô∏è QUAN TR·ªåNG: H√£y tham kh·∫£o h·ªì s∆° n√†y khi t∆∞ v·∫•n. 
- N·∫øu user D·ªä ·ª®NG v·ªõi thu·ªëc/th·ª±c ph·∫©m n√†o ‚Üí TUY·ªÜT ƒê·ªêI KH√îNG ƒë·ªÅ xu·∫•t
- N·∫øu c√≥ b·ªánh m√£n t√≠nh ‚Üí L∆∞u √Ω t∆∞∆°ng t√°c thu·ªëc v√† ch·∫ø ƒë·ªô ƒÉn
"""
                        logger.info(f"‚úì Loaded health profile for user {user_id}")
            except Exception as e:
                logger.warning(f"Could not load health profile from conversation: {e}")
        if not search_results:
            return {
                "answer": """Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong c∆° s·ªü d·ªØ li·ªáu y t·∫ø ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa b·∫°n.

‚ö†Ô∏è Khuy·∫øn c√°o: Vui l√≤ng tham kh·∫£o √Ω ki·∫øn b√°c sƒ© chuy√™n khoa ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c v√† an to√†n.""",
                "sources": [],
                "confidence": "none"
            }
        
        # Prepare context - PRIORITIZE ORIGINAL ANSWER from dataset
        context_parts = []
        for idx, result in enumerate(search_results[:3], 1):
            metadata = result['metadata']
            
            # Check if we have original Q&A from dataset (Excel/CSV)
            original_answer = metadata.get('original_answer', '')
            original_question = metadata.get('original_question', '')
            
            if original_answer and len(original_answer) > 50:
                # Use original answer from dataset (more accurate)
                context_parts.append(f"""
[Ngu·ªìn {idx}] {metadata.get('source', 'Medical Database')}
C√¢u h·ªèi g·ªëc: {original_question if original_question else metadata.get('disease_name', 'N/A')}
C√¢u tr·∫£ l·ªùi: {original_answer}
ƒê·ªô li√™n quan: {result.get('relevance_score', 0):.2f}
""")
            else:
                # Fallback to structured data
                context_parts.append(f"""
[Ngu·ªìn {idx}] B·ªánh: {metadata.get('disease_name', 'N/A')}
- Tri·ªáu ch·ª©ng: {metadata.get('symptoms', 'N/A')}
- ƒêi·ªÅu tr·ªã: {metadata.get('treatment', 'N/A')}
- Ph√≤ng ng·ª´a: {metadata.get('prevention', 'N/A')}
- ƒê·ªô li√™n quan: {result.get('relevance_score', 0):.2f}
""")
        context = "\n".join(context_parts)
        
        # Personalize greeting
        greeting_instruction = '- B·∫Øt ƒë·∫ßu b·∫±ng "Ch√†o b·∫°n,"'
        if user_name:
            greeting_instruction = f'- B·∫Øt ƒë·∫ßu b·∫±ng "Ch√†o b·∫°n {user_name},"'
        
        # Enhanced system prompt - EMPHASIZE SOURCE ACCURACY
        system_prompt = f"""
B·∫°n l√† B√°c sƒ© AI v·ªõi 10 nƒÉm kinh nghi·ªám l√¢m s√†ng, chuy√™n t∆∞ v·∫•n s·ª©c kh·ªèe cho ng∆∞·ªùi Vi·ªát Nam.

QUY T·∫ÆC B·∫ÆT BU·ªòC (QUAN TR·ªåNG NH·∫§T):
1. ‚úÖ S·ª¨ D·ª§NG CH√çNH X√ÅC th√¥ng tin t·ª´ [Ngu·ªìn] ƒë∆∞·ª£c cung c·∫•p
2. ‚úÖ ∆ØU TI√äN tr√≠ch d·∫´n "C√¢u tr·∫£ l·ªùi" g·ªëc t·ª´ ngu·ªìn (n·∫øu c√≥)
3. ‚úÖ C√≥ th·ªÉ di·ªÖn ƒë·∫°t l·∫°i cho t·ª± nhi√™n NH∆ØNG KH√îNG thay ƒë·ªïi n·ªôi dung
4. ‚ùå TUY·ªÜT ƒê·ªêI KH√îNG t·ª± suy lu·∫≠n ho·∫∑c th√™m th√¥ng tin kh√¥ng c√≥ trong ngu·ªìn
5. ‚ùå KH√îNG ch·∫©n ƒëo√°n ch·∫Øc ch·∫Øn (d√πng "c√≥ th·ªÉ", "kh·∫£ nƒÉng")
6. ‚ùå KH√îNG k√™ ƒë∆°n thu·ªëc c·ª• th·ªÉ

{health_profile_context if health_profile_context else ""}

C√ÅCH TR·∫¢ L·ªúI:
{greeting_instruction}
- Tr·∫£ l·ªùi D·ª∞A TR√äN n·ªôi dung t·ª´ [Ngu·ªìn]
- N·∫øu ngu·ªìn c√≥ "C√¢u tr·∫£ l·ªùi" g·ªëc ‚Üí D√πng n·ªôi dung ƒë√≥ (c√≥ th·ªÉ t√≥m t·∫Øt n·∫øu qu√° d√†i)
- N·∫øu ch·ªâ c√≥ th√¥ng tin c·∫•u tr√∫c ‚Üí T·ªïng h·ª£p t·ª´ Tri·ªáu ch·ª©ng, ƒêi·ªÅu tr·ªã, Ph√≤ng ng·ª´a
- Chia th√†nh 2-3 ƒëo·∫°n ng·∫Øn, d√πng bullet points (‚Ä¢)
- Gi·ªçng ƒëi·ªáu th√¢n thi·ªán, kh√¥ng g√¢y ho·∫£ng lo·∫°n

LU√îN KHUY·∫æN C√ÅO ƒêI KH√ÅM B√ÅC Sƒ® N·∫æU:
‚Ä¢ Tri·ªáu ch·ª©ng k√©o d√†i > 3 ng√†y
‚Ä¢ S·ªët cao > 39¬∞C
‚Ä¢ C√≥ d·∫•u hi·ªáu nguy hi·ªÉm: kh√≥ th·ªü, ƒëau ng·ª±c, co gi·∫≠t

V√ç D·ª§ TR·∫¢ L·ªúI T·ªêT:
"Ch√†o b·∫°n {user_name if user_name else ''}, 

Theo th√¥ng tin t·ª´ ngu·ªìn y t·∫ø, t√¨nh tr·∫°ng k√©m ƒÉn c·ªßa b√© c≈©ng c√≥ th·ªÉ do b√© ƒëang m·ªçc rƒÉng ho·∫∑c do b√© ƒëang b·ªánh. Tuy nhi√™n, n·∫øu b√© v·ª´a s·ª≠ d·ª•ng kh√°ng sinh xong m√† v·∫´n c√≤n s·ªët, ho v√† b·ª•ng ch∆∞·ªõng cƒÉng, b·∫°n n√™n ƒë∆∞a b√© ƒë·∫øn c∆° s·ªü y t·∫ø g·∫ßn nh·∫•t c√≥ chuy√™n khoa Nhi ƒë·ªÉ thƒÉm kh√°m v√† l√†m c√°c x√©t nghi·ªám c·∫ßn thi·∫øt nh√©."
"""
        
        
        # Build user prompt with conversation context
        user_prompt_parts = [f"C√¢u h·ªèi hi·ªán t·∫°i: {question}"]
        
        # Add conversation summary if available
        if conversation_summary:
            user_prompt_parts.append(f"""
„ÄêT√≥m t·∫Øt cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥„Äë
{conversation_summary}""")

        # Add conversation history if available
        if conversation_context:
            user_prompt_parts.append(f"""
„ÄêL·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y„Äë
{conversation_context}

‚ö†Ô∏è L∆ØU √ù: H√£y tham kh·∫£o l·ªãch s·ª≠ ƒë·ªÉ hi·ªÉu ng·ªØ c·∫£nh. 
V√≠ d·ª•: N·∫øu user h·ªèi "c√≤n c√°ch n√†o kh√°c?" th√¨ "c√°ch" ƒë√≥ ƒë√£ ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p tr∆∞·ªõc ƒë√≥.""")
        
        user_prompt_parts.append(f"""
„ÄêTh√¥ng tin y t·∫ø t·ª´ c∆° s·ªü d·ªØ li·ªáu„Äë
{context}

„ÄêTh√¥ng tin tr√≠ch xu·∫•t„Äë
{json.dumps(extracted_features, ensure_ascii=False)}

H√£y tr·∫£ l·ªùi theo ƒë√∫ng quy t·∫Øc.""")
        
        user_prompt = "\n\n".join(user_prompt_parts)
        
        # === TOOL CALLING: Cho ph√©p GPT g·ªçi functions ===
        # GPT s·∫Ω T·ª∞ QUY·∫æT ƒê·ªäNH khi n√†o c·∫ßn g·ªçi tool (v√≠ d·ª•: t√¨m b·ªánh vi·ªán)
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # G·ªçi GPT l·∫ßn ƒë·∫ßu (c√≥ th·ªÉ trigger tool call)
        response = client.chat.completions.create(
            model="gpt-4o",  # gpt-4o h·ªó tr·ª£ tool calling t·ªët h∆°n gpt-4o-mini
            messages=messages,
            tools=AVAILABLE_TOOLS,  # Danh s√°ch tools GPT c√≥ th·ªÉ g·ªçi
            tool_choice="auto",  # GPT t·ª± quy·∫øt ƒë·ªãnh khi n√†o g·ªçi tool
            temperature=0.3,
            max_tokens=800
        )
        
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls
        
        # Ki·ªÉm tra xem GPT c√≥ g·ªçi tool kh√¥ng
        if tool_calls:
            logger.info(f"üîß GPT triggered {len(tool_calls)} tool call(s)")
            
            # Th√™m response c·ªßa GPT v√†o messages
            messages.append(response_message)
            
            # Th·ª±c thi t·ª´ng tool call
            for tool_call in tool_calls:
                function_name = tool_call.function.name
                logger.info(f"Executing tool: {function_name}")
                
                # G·ªçi function v√† l·∫•y k·∫øt qu·∫£
                function_response = execute_tool_call(tool_call)
                
                # Th√™m k·∫øt qu·∫£ v√†o messages
                messages.append({
                    "tool_call_id": tool_call.id,
                    "role": "tool",
                    "name": function_name,
                    "content": function_response
                })
            
            # G·ªçi GPT l·∫ßn 2 ƒë·ªÉ t·ªïng h·ª£p k·∫øt qu·∫£ t·ª´ tool
            second_response = client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.3,
                max_tokens=800
            )
            
            answer = second_response.choices[0].message.content
            logger.info("‚úì Tool calling completed, final answer generated")
        else:
            # Kh√¥ng c√≥ tool call, l·∫•y answer tr·ª±c ti·∫øp
            answer = response_message.content
        
        # Add safety disclaimer if needed
        if "b√°c sƒ©" not in answer.lower() and "kh√°m" not in answer.lower():
            answer += "\n\n‚ö†Ô∏è L∆∞u √Ω: Th√¥ng tin tr√™n ch·ªâ mang t√≠nh ch·∫•t tham kh·∫£o. Vui l√≤ng tham kh·∫£o √Ω ki·∫øn b√°c sƒ© chuy√™n khoa."
        
        avg_score = np.mean([r.get('relevance_score', 0) for r in search_results[:3]])
        confidence = 'high' if avg_score > 0.7 else 'medium' if avg_score > 0.5 else 'low'
        
        logger.info(f"Response generated successfully (confidence: {confidence})")
        
        return {
            "answer": answer,
            "sources": search_results[:3],
            "confidence": confidence,
            "avg_relevance_score": round(avg_score, 3)
        }
        
    except Exception as e:
        logger.error(f"Error generating response: {str(e)}", exc_info=True)
        return {
            "answer": """Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t. Vui l√≤ng th·ª≠ l·∫°i sau.

‚ö†Ô∏è N·∫øu b·∫°n ƒëang g·∫∑p v·∫•n ƒë·ªÅ s·ª©c kh·ªèe c·∫•p b√°ch, vui l√≤ng li√™n h·ªá b√°c sƒ© ngay l·∫≠p t·ª©c.""",
            "error": str(e),
            "sources": [],
            "confidence": "error"
        }
