import os
import json
from dotenv import load_dotenv  # Import ƒë·ªÉ load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env (VD: API Key)

# Load bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()
from openai import OpenAI  # Import th∆∞ vi·ªán OpenAI ƒë·ªÉ g·ªçi GPT
from typing import Dict, List, Optional, Any, Tuple  # Import Type Hinting ƒë·ªÉ code r√µ r√†ng h∆°n
import chromadb  # Import ChromaDB - Database Vector ƒë·ªÉ l∆∞u tr·ªØ ki·∫øn th·ª©c y t·∫ø
import numpy as np  # Import numpy ƒë·ªÉ t√≠nh to√°n vector
import sys
import logging
import re
from collections import defaultdict  # Import defaultdict ƒë·ªÉ d·ªÖ d√†ng gom nh√≥m k·∫øt qu·∫£ t√¨m ki·∫øm

# C·∫•u h√¨nh logging ƒë·ªÉ theo d√µi ho·∫°t ƒë·ªông h·ªá th·ªëng
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Th√™m ƒë∆∞·ªùng d·∫´n src v√†o system path ƒë·ªÉ import c√°c module kh√°c
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(src_dir)

from src.nlp_model.phobert_embedding import PhoBERTEmbeddingFunction  # Import model PhoBERT ƒë·ªÉ chuy·ªÉn vƒÉn b·∫£n th√†nh Vector
from src.services.bm25_search import BM25SearchEngine, create_searchable_text  # Import c√¥ng c·ª• t√¨m ki·∫øm t·ª´ kh√≥a BM25
from src.services.hospital_finder_service import hospital_finder_service  # Service t√¨m b·ªánh vi·ªán
from src.services.tool_calling_functions import AVAILABLE_TOOLS, execute_tool_call  # C√°c h√†m h·ªó tr·ª£ Agent g·ªçi tool

# Import Cross-Encoder ƒë·ªÉ s·∫Øp x·∫øp l·∫°i k·∫øt qu·∫£ (Reranking) - Gi√∫p tƒÉng ƒë·ªô ch√≠nh x√°c
try:
    from sentence_transformers import CrossEncoder
    # S·ª≠ d·ª•ng model MS-MARCO MiniLM v√¨ n√≥ nh·∫π v√† hi·ªáu qu·∫£ cho vi·ªác rerank
    RERANKER = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    RERANKING_ENABLED = True
    logger.info("‚úì Cross-Encoder loaded for reranking")
except ImportError:
    RERANKING_ENABLED = False
    logger.warning("‚ö† sentence-transformers not installed. Reranking disabled.")

# Kh·ªüi t·∫°o b·ªô t√¨m ki·∫øm BM25 (t√¨m ki·∫øm theo t·ª´ kh√≥a)
BM25_ENGINE = BM25SearchEngine()
BM25_ENABLED = False  # S·∫Ω ƒë∆∞·ª£c set th√†nh True sau khi load d·ªØ li·ªáu xong

# Kh·ªüi t·∫°o OpenAI Client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Kh·ªüi t·∫°o ChromaDB Client (L∆∞u tr·ªØ Vector)
workspace_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
chroma_client = chromadb.PersistentClient(path=os.path.join(workspace_root, 'src', 'nlp_model', 'data', 'chroma_db'))

# Kh·ªüi t·∫°o h√†m Embedding PhoBERT (D√πng cho ti·∫øng Vi·ªát)
phobert_ef = PhoBERTEmbeddingFunction()

# Danh s√°ch t·ª´ kh√≥a y t·∫ø quan tr·ªçng ƒë·ªÉ t√≠nh ƒëi·ªÉm li√™n quan
MEDICAL_KEYWORDS = {
    'symptoms': ['tri·ªáu ch·ª©ng', 'd·∫•u hi·ªáu', 'bi·ªÉu hi·ªán', 's·ªët', 'ho', 'ƒëau', 'ng·ª©a', 'm·ªát', 'bu·ªìn n√¥n'],
    'treatment': ['ƒëi·ªÅu tr·ªã', 'ch·ªØa', 'thu·ªëc', 'u·ªëng', 'd√πng', 'kh√°m', 'b√°c sƒ©'],
    'prevention': ['ph√≤ng ng·ª´a', 'tr√°nh', 'v·ªá sinh', 'v·∫Øc-xin', 'ti√™m ch·ªßng'],
    'diagnosis': ['ch·∫©n ƒëo√°n', 'x√©t nghi·ªám', 'ki·ªÉm tra', 'kh√°m']
}

# Ng∆∞·ª°ng tin c·∫≠y (Confidence Threshold)
# N·∫øu ƒëi·ªÉm s·ªë th·∫•p h∆°n ng∆∞·ª°ng n√†y th√¨ coi nh∆∞ kh√¥ng li√™n quan
CONFIDENCE_THRESHOLD = 0.10  # ƒê√£ h·∫° th·∫•p xu·ªëng 0.10 ƒë·ªÉ l·∫•y ƒë∆∞·ª£c nhi·ªÅu k·∫øt qu·∫£ h∆°n

# Tr·ªçng s·ªë cho Hybrid Search (K·∫øt h·ª£p BM25 v√† Vector)
# 70% ƒëi·ªÉm s·ªë d·ª±a tr√™n t·ª´ kh√≥a (BM25) - Quan tr·ªçng v√¨ thu·∫≠t ng·ªØ y t·∫ø c·∫ßn ch√≠nh x√°c
# 30% ƒëi·ªÉm s·ªë d·ª±a tr√™n ng·ªØ nghƒ©a (Vector) - Gi√∫p t√¨m c√°c t·ª´ ƒë·ªìng nghƒ©a
HYBRID_BM25_WEIGHT = 0.7
HYBRID_VECTOR_WEIGHT = 0.3

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PH·∫¶N 1: T·ªêI ∆ØU H√ìA RAG (Query Expansion & Reranking)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def expand_query(question: str) -> List[str]:
    """
    K·ªπ thu·∫≠t Query Expansion: M·ªü r·ªông c√¢u h·ªèi c·ªßa user th√†nh nhi·ªÅu c√¢u t∆∞∆°ng t·ª±.
    Gi√∫p t√¨m ki·∫øm ƒë∆∞·ª£c nhi·ªÅu k·∫øt qu·∫£ h∆°n n·∫øu user d√πng t·ª´ kh√¥ng chu·∫©n.
    
    VD: "ƒëau ƒë·∫ßu" -> ["ƒëau ƒë·∫ßu l√† g√¨", "nguy√™n nh√¢n g√¢y nh·ª©c ƒë·∫ßu", "ƒëau ƒë·∫ßu"]
    """
    try:
        # Prompt nh·ªù GPT t·∫°o ra 2 c√¢u h·ªèi t∆∞∆°ng t·ª±
        prompt = f"""B·∫°n l√† chuy√™n gia y t·∫ø. H√£y t·∫°o 2 c√¢u h·ªèi T∆Ø∆†NG T·ª∞ (kh√¥ng gi·ªëng h·ªát) v·ªõi c√¢u h·ªèi g·ªëc.

C√¢u h·ªèi g·ªëc: "{question}"

Y√™u c·∫ßu:
- Gi·ªØ nguy√™n √Ω nghƒ©a y t·∫ø
- D√πng t·ª´ ƒë·ªìng nghƒ©a ho·∫∑c c√°ch di·ªÖn ƒë·∫°t kh√°c
- M·ªói c√¢u tr√™n 1 d√≤ng
- KH√îNG gi·∫£i th√≠ch, CH·ªà tr·∫£ v·ªÅ 2 c√¢u h·ªèi

V√≠ d·ª•:
C√¢u g·ªëc: "S·ªët cao l√† bao nhi√™u ƒë·ªô?"
C√¢u 1: Nhi·ªát ƒë·ªô c∆° th·ªÉ bao nhi√™u ƒë∆∞·ª£c coi l√† s·ªët cao?
C√¢u 2: S·ªët tr√™n bao nhi√™u ƒë·ªô C l√† nguy hi·ªÉm?
"""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # D√πng model nh·ªè cho nhanh v√† r·∫ª
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=150
        )
        
        expanded_text = response.choices[0].message.content.strip()
        expanded_queries = [q.strip() for q in expanded_text.split('\n') if q.strip()]
        
        # Lu√¥n ƒë∆∞a c√¢u h·ªèi g·ªëc l√™n ƒë·∫ßu ti√™n
        all_queries = [question] + expanded_queries[:2]
        logger.info(f"Query expansion: {question} ‚Üí {len(all_queries)} queries")
        return all_queries
        
    except Exception as e:
        logger.warning(f"Query expansion failed: {e}. Using original query only.")
        return [question]

def generate_search_query_from_image(image_base64: str) -> str:
    """
    D√πng GPT-4o Vision ƒë·ªÉ nh√¨n ·∫£nh v√† sinh ra t·ª´ kh√≥a t√¨m ki·∫øm.
    VD: ·∫¢nh ch·ª•p v·∫øt th∆∞∆°ng -> GPT tr·∫£ v·ªÅ "v·∫øt th∆∞∆°ng h·ªü, s∆∞ng t·∫•y"
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "H√£y nh√¨n b·ª©c ·∫£nh n√†y v√† li·ªát k√™ 3-5 t·ª´ kh√≥a y t·∫ø quan tr·ªçng nh·∫•t b·∫±ng ti·∫øng Vi·ªát ƒë·ªÉ t√¨m ki·∫øm trong c∆° s·ªü d·ªØ li·ªáu. V√ç D·ª§: 'm·∫©n ƒë·ªè ng·ª©a', 'v·∫øt th∆∞∆°ng h·ªü', 's∆∞ng t·∫•y'. CH·ªà TR·∫¢ V·ªÄ T·ª™ KH√ìA, KH√îNG GI·∫¢I TH√çCH."},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": image_base64 if image_base64.startswith("data:image") else f"data:image/jpeg;base64,{image_base64}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=50
        )
        keywords = response.choices[0].message.content.strip()
        logger.info(f"üñºÔ∏è Image keywords extracted: {keywords}")
        return keywords
    except Exception as e:
        logger.error(f"Failed to extract keywords from image: {e}")
        return ""

def rewrite_query_with_context(question: str, conversation_id: int) -> str:
    """
    Vi·∫øt l·∫°i c√¢u h·ªèi d·ª±a tr√™n l·ªãch s·ª≠ chat (Contextual Rewriting).
    VD: 
       User: "B·ªánh ti·ªÉu ƒë∆∞·ªùng l√† g√¨?"
       Bot: "..."
       User: "N√≥ c√≥ nguy hi·ªÉm kh√¥ng?" -> Vi·∫øt l·∫°i th√†nh "B·ªánh ti·ªÉu ƒë∆∞·ªùng c√≥ nguy hi·ªÉm kh√¥ng?"
    """
    try:
        from src.models.message import Message
        
        # L·∫•y 2 tin nh·∫Øn g·∫ßn nh·∫•t ƒë·ªÉ hi·ªÉu ng·ªØ c·∫£nh
        recent_messages = Message.query.filter_by(
            conversation_id=conversation_id
        ).order_by(Message.sent_at.desc()).limit(2).all()
        
        if not recent_messages:
            return question
            
        recent_messages.reverse()
        history_text = "\n".join([f"{'User' if m.sender=='user' else 'Bot'}: {m.message_text}" for m in recent_messages])
        
        prompt = f"""H√£y vi·∫øt l·∫°i c√¢u h·ªèi cu·ªëi c√πng c·ªßa User ƒë·ªÉ n√≥ ƒê·∫¶Y ƒê·ª¶ √ù NGHƒ®A, d·ª±a v√†o ng·ªØ c·∫£nh tr∆∞·ªõc ƒë√≥.

L·ªãch s·ª≠:
{history_text}

C√¢u h·ªèi hi·ªán t·∫°i: "{question}"

Y√™u c·∫ßu:
- N·∫øu c√¢u h·ªèi ƒë√£ r√µ r√†ng, gi·ªØ nguy√™n.
- N·∫øu c√¢u h·ªèi thi·∫øu ch·ªß ng·ªØ/ng·ªØ c·∫£nh (v√≠ d·ª•: "N√≥ l√† g√¨?", "U·ªëng thu·ªëc g√¨?"), h√£y th√™m t√™n b·ªánh/v·∫•n ƒë·ªÅ t·ª´ l·ªãch s·ª≠ v√†o.
- CH·ªà tr·∫£ v·ªÅ c√¢u h·ªèi ƒë√£ vi·∫øt l·∫°i (ho·∫∑c c√¢u g·ªëc). KH√îNG gi·∫£i th√≠ch.

C√¢u h·ªèi vi·∫øt l·∫°i:"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3, # Nhi·ªát ƒë·ªô th·∫•p ƒë·ªÉ ch√≠nh x√°c
            max_tokens=100
        )
        
        rewritten = response.choices[0].message.content.strip()
        logger.info(f"üîÑ Rewrote query: '{question}' -> '{rewritten}'")
        return rewritten
        
    except Exception as e:
        logger.warning(f"Query rewrite failed: {e}")
        return question

def rerank_results(question: str, results: List[Dict]) -> List[Dict]:
    """
    S·∫Øp x·∫øp l·∫°i k·∫øt qu·∫£ t√¨m ki·∫øm (Reranking) b·∫±ng Cross-Encoder.
    Cross-Encoder so s√°nh tr·ª±c ti·∫øp c√¢u h·ªèi v√† vƒÉn b·∫£n ƒë·ªÉ ch·∫•m ƒëi·ªÉm ch√≠nh x√°c h∆°n Vector Search.
    """
    if not RERANKING_ENABLED or not results:
        return results
    
    try:
        # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ ƒë∆∞a v√†o model: List c√°c c·∫∑p [C√¢u h·ªèi, VƒÉn b·∫£n]
        pairs = []
        for result in results:
            # Gom t·∫•t c·∫£ th√¥ng tin trong metadata th√†nh 1 ƒëo·∫°n vƒÉn
            doc_text = f"{result['metadata'].get('disease_name', '')} "
            doc_text += f"{result['metadata'].get('symptoms', '')} "
            doc_text += f"{result['metadata'].get('treatment', '')}"
            pairs.append([question, doc_text])
        
        # Ch·∫•m ƒëi·ªÉm
        ce_scores = RERANKER.predict(pairs)
        
        # G√°n ƒëi·ªÉm m·ªõi v√† t√≠nh ƒëi·ªÉm t·ªïng h·ª£p cu·ªëi c√πng
        for i, result in enumerate(results):
            result['ce_score'] = float(ce_scores[i])
            # ƒêi·ªÉm cu·ªëi c√πng = 70% Rerank Score + 30% Original Score
            result['final_score'] = 0.7 * ce_scores[i] + 0.3 * result.get('relevance_score', 0)
        
        # S·∫Øp x·∫øp l·∫°i danh s√°ch theo ƒëi·ªÉm final_score gi·∫£m d·∫ßn
        reranked = sorted(results, key=lambda x: x['final_score'], reverse=True)
        
        logger.info(f"Reranked {len(results)} results. Top score: {reranked[0]['final_score']:.3f}")
        return reranked
        
    except Exception as e:
        logger.error(f"Reranking failed: {e}. Using original order.")
        return results

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# T√ìM T·∫ÆT H·ªòI THO·∫†I
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def generate_conversation_summary(conversation_id: int) -> Optional[str]:
    """H√†m t√≥m t·∫Øt n·ªôi dung cu·ªôc tr√≤ chuy·ªán ƒë·ªÉ l∆∞u v√†o DB (hi·ªÉn th·ªã ·ªü m√†n h√¨nh danh s√°ch)"""
    try:
        from src.models.message import Message
        
        messages = Message.query.filter_by(
            conversation_id=conversation_id
        ).order_by(Message.sent_at).all()
        
        if not messages or len(messages) < 3:
            return None
        
        conversation_text = []
        for msg in messages:
            sender = "Ng∆∞·ªùi d√πng" if msg.sender == 'user' else "B√°c sƒ© AI"
            conversation_text.append(f"{sender}: {msg.message_text}")
        
        full_conversation = "\n".join(conversation_text)
        
        prompt = f"""B·∫°n l√† tr·ª£ l√Ω y t·∫ø. H√£y t√≥m t·∫Øt cu·ªôc h·ªôi tho·∫°i sau th√†nh 3-5 d√≤ng NG·∫ÆN G·ªåN.

Cu·ªôc h·ªôi tho·∫°i:
{full_conversation}

Y√äU C·∫¶U T√ìM T·∫ÆT:
- Ch·ªâ ghi c√°c th√¥ng tin Y T·∫æ quan tr·ªçng
- Format: Bullet points (‚Ä¢)
- Bao g·ªìm: Tri·ªáu ch·ª©ng, thu·ªëc ƒë√£ d√πng, t√¨nh tr·∫°ng hi·ªán t·∫°i
- KH√îNG gi·∫£i th√≠ch, CH·ªà li·ªát k√™ th√¥ng tin

H√£y t√≥m t·∫Øt:"""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=200
        )
        
        summary = response.choices[0].message.content.strip()
        return summary
        
    except Exception as e:
        logger.error(f"Failed to generate summary: {e}")
        return None

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C√ÅC H√ÄM H·ªñ TR·ª¢ VECTOR DB & T√çNH TO√ÅN ƒêI·ªÇM S·ªê
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def get_or_create_collection():
    """L·∫•y ho·∫∑c t·∫°o Collection trong ChromaDB"""
    try:
        collection = chroma_client.get_collection(
            name="medical_collection",
            embedding_function=phobert_ef
        )
        return collection
    except Exception as e:
        print(f"Collection not found, creating new one: {str(e)}")
        collection = chroma_client.create_collection(
            name="medical_collection",
            embedding_function=phobert_ef
        )
        return collection

def initialize_bm25_index():
    """
    Kh·ªüi t·∫°o ch·ªâ m·ª•c BM25 t·ª´ to√†n b·ªô d·ªØ li·ªáu trong ChromaDB.
    H√†m n√†y c·∫ßn ch·∫°y 1 l·∫ßn khi server kh·ªüi ƒë·ªông.
    """
    global BM25_ENABLED
    
    try:
        logger.info("Initializing BM25 index...")
        collection = get_or_create_collection()
        
        # L·∫•y to√†n b·ªô d·ªØ li·ªáu (documents v√† metadata)
        all_docs = collection.get(
            include=["documents", "metadatas"]
        )
        
        if not all_docs['ids']:
            logger.warning("No documents found in ChromaDB. BM25 index not created.")
            return False
        
        # T·∫°o vƒÉn b·∫£n searchable t·ª´ metadata (k·∫øt h·ª£p t√™n b·ªánh, tri·ªáu ch·ª©ng...)
        searchable_texts = [
            create_searchable_text(metadata) 
            for metadata in all_docs['metadatas']
        ]
        
        # Index d·ªØ li·ªáu v√†o BM25 Engine
        BM25_ENGINE.index_documents(
            documents=searchable_texts,
            document_ids=all_docs['ids'],
            metadatas=all_docs['metadatas']
        )
        
        BM25_ENABLED = True
        logger.info(f"‚úì BM25 index initialized with {len(all_docs['ids'])} documents")
        return True
        
    except Exception as e:
        logger.error(f"Failed to initialize BM25 index: {e}")
        BM25_ENABLED = False
        return False

def normalize_similarity(distance: float) -> float:
    """Chuy·ªÉn ƒë·ªïi kho·∫£ng c√°ch L2 (Distance) th√†nh ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng (Similarity Score 0-1)"""
    if distance <= 0:
        return 1.0
    # C√¥ng th·ª©c: 1 / (1 + distance)
    sim = 1 / (1 + (distance / 10))
    return float(sim)

def extract_keywords(text: str) -> List[str]:
    """T√°ch t·ª´ kh√≥a t·ª´ m·ªôt ƒëo·∫°n vƒÉn (b·ªè c√°c t·ª´ n·ªëi stop_words)"""
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    words = text.split()
    stop_words = {'l√†', 'c·ªßa', 'v√†', 'c√≥', 'ƒë∆∞·ª£c', 'n√†y', 'ƒë√≥', 'c√°c', 'cho', 't·ª´', 'v·ªõi', 'm·ªôt', 'nh·ªØng'}
    keywords = [w for w in words if w not in stop_words and len(w) > 2]
    return keywords

def calculate_keyword_match_score(question: str, document: str, metadata: Dict) -> float:
    """T√≠nh ƒëi·ªÉm kh·ªõp t·ª´ kh√≥a (Keyword Match) gi·ªØa c√¢u h·ªèi v√† vƒÉn b·∫£n"""
    question_keywords = set(extract_keywords(question))
    
    # T·∫°o vƒÉn b·∫£n t·ªïng h·ª£p c·ªßa document
    searchable_text = ' '.join([
        str(metadata.get('disease_name', '')),
        str(metadata.get('symptoms', '')),
        str(metadata.get('treatment', '')),
        str(metadata.get('prevention', '')),
        str(metadata.get('description', ''))
    ]).lower()
    
    doc_keywords = set(extract_keywords(searchable_text))
    if not question_keywords or not doc_keywords:
        return 0.0
    
    # T√≠nh giao thoa (Jaccard Similarity)
    intersection = len(question_keywords & doc_keywords)
    union = len(question_keywords | doc_keywords)
    return intersection / union if union > 0 else 0.0

def calculate_medical_relevance_score(question: str, metadata: Dict) -> float:
    """T√≠nh ƒëi·ªÉm c·ªông th√™m n·∫øu kh·ªõp ƒë√∫ng ng·ªØ c·∫£nh y t·∫ø (tri·ªáu ch·ª©ng, ƒëi·ªÅu tr·ªã...)"""
    question_lower = question.lower()
    score = 0.0
    for category, keywords in MEDICAL_KEYWORDS.items():
        if any(kw in question_lower for kw in keywords):
            # N·∫øu c√¢u h·ªèi ch·ª©a t·ª´ kh√≥a lo·∫°i n√†o (VD: "ƒëi·ªÅu tr·ªã"), ki·ªÉm tra xem document c√≥ tr∆∞·ªùng ƒë√≥ kh√¥ng
            field_value = str(metadata.get(category, '')).lower()
            if field_value and len(field_value) > 5:
                score += 0.15 # C·ªông ƒëi·ªÉm th∆∞·ªüng
    return min(score, 0.6)

def calculate_combined_score(
    distance: float,
    question: str,
    document: str,
    metadata: Dict
) -> Tuple[float, Dict[str, float]]:
    """T√≠nh ƒëi·ªÉm t·ªïng h·ª£p t·ª´ c√°c th√†nh ph·∫ßn (Semantic + Keyword + Medical Context)"""
    semantic_score = normalize_similarity(distance)
    keyword_score = calculate_keyword_match_score(question, document, metadata)
    medical_score = calculate_medical_relevance_score(question, metadata)
    
    final_score = (
        0.5 * semantic_score +
        0.3 * keyword_score +
        0.2 * medical_score
    )
    score_breakdown = {
        'semantic': round(semantic_score, 3),
        'keyword': round(keyword_score, 3),
        'medical': round(medical_score, 3),
        'final': round(final_score, 3)
    }
    return final_score, score_breakdown

def extract_user_intent_and_features(question: str) -> Dict[str, Any]:
    """
    D√πng GPT ƒë·ªÉ ph√¢n t√≠ch √Ω ƒë·ªãnh ng∆∞·ªùi d√πng (User Intent).
    Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ nh∆∞: T√™n b·ªánh, Tri·ªáu ch·ª©ng, Thu·ªëc...
    """
    tools_schema = [
        {
            "type": "function",
            "function": {
                "name": "tim_kiem_thong_tin_y_te",
                "description": "Tr√≠ch xu·∫•t th√¥ng tin y t·∫ø t·ª´ c√¢u h·ªèi ng∆∞·ªùi d√πng",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "trieu_chung": {"type": "string", "description": "C√°c tri·ªáu ch·ª©ng b·ªánh"},
                        "ten_benh": {"type": "string", "description": "T√™n b·ªánh nghi ng·ªù"},
                        "thuoc": {"type": "string", "description": "T√™n thu·ªëc ho·∫∑c lo·∫°i thu·ªëc"},
                        "muc_dich": {"type": "string", "description": "M·ª•c ƒë√≠ch h·ªèi"}
                    },
                    "required": []
                }
            }
        }
    ]
    system_prompt = "B·∫°n l√† tr·ª£ l√Ω y t·∫ø AI. H√£y ph√¢n t√≠ch c√¢u h·ªèi v√† tr√≠ch xu·∫•t th√¥ng tin y t·∫ø quan tr·ªçng."
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            tools=tools_schema,
            tool_choice={"type": "function", "function": {"name": "tim_kiem_thong_tin_y_te"}}
        )
        tool_calls = response.choices[0].message.tool_calls
        if tool_calls:
            function_args = json.loads(tool_calls[0].function.arguments)
            return {
                "original_question": question,
                "intent": "tim_kiem_thong_tin_y_te",
                "confidence": 0.9,
                "extracted_features": function_args
            }
        return {
            "original_question": question,
            "intent": "general_question",
            "confidence": 0.5,
            "extracted_features": {}
        }
    except Exception as e:
        print(f"Error in extract_user_intent_and_features: {str(e)}")
        return {
            "original_question": question,
            "intent": "error",
            "confidence": 0.0,
            "extracted_features": {}
        }

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# C∆† CH·∫æ T√åM KI·∫æM CH√çNH (HYBRID SEARCH)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def hybrid_search(
    question: str,
    n_results: int = 10
) -> List[Dict[str, Any]]:
    """
    T√¨m ki·∫øm k·∫øt h·ª£p (Hybrid Search): BM25 + Vector.
    Output: Danh s√°ch k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c ch·∫•m ƒëi·ªÉm t·ªïng h·ª£p.
    """
    results_dict = defaultdict(lambda: {'bm25_score': 0.0, 'vector_score': 0.0})
    
    # 1. T√åM KI·∫æM KEYWORD (BM25)
    if BM25_ENABLED:
        try:
            bm25_results = BM25_ENGINE.search(question, top_k=n_results * 2)
            
            # Chu·∫©n h√≥a ƒëi·ªÉm BM25 v·ªÅ kho·∫£ng 0-1 ƒë·ªÉ c·ªông v·ªõi ƒëi·ªÉm Vector
            if bm25_results:
                max_bm25 = max(r['bm25_score'] for r in bm25_results)
                if max_bm25 > 0:
                    for result in bm25_results:
                        doc_id = result['id']
                        normalized_score = result['bm25_score'] / max_bm25
                        results_dict[doc_id]['bm25_score'] = normalized_score
                        results_dict[doc_id]['metadata'] = result['metadata']
                        results_dict[doc_id]['document'] = result['document']
                        results_dict[doc_id]['id'] = doc_id
            
            logger.info(f"BM25 found {len(bm25_results)} results")
        except Exception as e:
            logger.error(f"BM25 search failed: {e}")
    
    # 2. T√åM KI·∫æM NG·ªÆ NGHƒ®A (VECTOR SEARCH)
    try:
        collection = get_or_create_collection()
        query_vec = phobert_ef([question])[0] # M√£ h√≥a c√¢u h·ªèi th√†nh Vector
        vector_results = collection.query(
            query_embeddings=[query_vec],
            n_results=n_results * 2,
            include=["metadatas", "documents", "distances"]
        )
        
        # X·ª≠ l√Ω k·∫øt qu·∫£ Vector
        for i in range(len(vector_results['ids'][0])):
            doc_id = vector_results['ids'][0][i]
            distance = vector_results['distances'][0][i]
            
            # Chu·∫©n h√≥a kho·∫£ng c√°ch th√†nh ƒëi·ªÉm Similarity (0-1)
            vector_score = normalize_similarity(distance)
            
            results_dict[doc_id]['vector_score'] = vector_score
            results_dict[doc_id]['metadata'] = vector_results['metadatas'][0][i]
            results_dict[doc_id]['document'] = vector_results['documents'][0][i]
            results_dict[doc_id]['id'] = doc_id
            results_dict[doc_id]['distance'] = distance
        
        logger.info(f"Vector search found {len(vector_results['ids'][0])} results")
    except Exception as e:
        logger.error(f"Vector search failed: {e}")
        return []
    
    # 3. K·∫æT H·ª¢P ƒêI·ªÇM S·ªê (COMBINE)
    combined_results = []
    for doc_id, scores in results_dict.items():
        # T√≠nh ƒëi·ªÉm Hybrid theo tr·ªçng s·ªë
        hybrid_score = (
            HYBRID_BM25_WEIGHT * scores['bm25_score'] + 
            HYBRID_VECTOR_WEIGHT * scores['vector_score']
        )
        
        combined_results.append({
            'id': scores['id'],
            'metadata': scores['metadata'],
            'document': scores['document'],
            'bm25_score': scores['bm25_score'],
            'vector_score': scores['vector_score'],
            'hybrid_score': hybrid_score,
            'relevance_score': hybrid_score,  # Gi·ªØ t√™n n√†y ƒë·ªÉ t∆∞∆°ng th√≠ch
            'distance': scores.get('distance', 0),
            'score_breakdown': {
                'bm25': round(scores['bm25_score'], 3),
                'vector': round(scores['vector_score'], 3),
                'hybrid': round(hybrid_score, 3)
            }
        })
    
    # S·∫Øp x·∫øp theo ƒëi·ªÉm Hybrid gi·∫£m d·∫ßn
    combined_results.sort(key=lambda x: x['hybrid_score'], reverse=True)
    
    return combined_results[:n_results]

def combined_search_with_filters(
    question: str,
    extracted_features: Dict[str, Any],
    n_results: int = 10
) -> Dict[str, Any]:
    """
    H√†m t√¨m ki·∫øm CAO C·∫§P: K·∫øt h·ª£p t·∫•t c·∫£ k·ªπ thu·∫≠t.
    1. Query Expansion (M·ªü r·ªông c√¢u h·ªèi)
    2. Hybrid Search (BM25 + Vector) cho m·ªói c√¢u h·ªèi
    3. Merge & Deduplicate (G·ªôp k·∫øt qu·∫£)
    4. Reranking (S·∫Øp x·∫øp l·∫°i b·∫±ng Cross-Encoder)
    """
    try:
        logger.info(f"üîç Hybrid search for: {question}")
        collection = get_or_create_collection()
        count = collection.count()
        if count == 0:
            logger.warning("No data in database")
            return {"success": False, "message": "No data in database", "results": []}
        
        # === B∆Ø·ªöC 1: QUERY EXPANSION ===
        expanded_queries = expand_query(question)
        logger.info(f"Expanded to {len(expanded_queries)} queries")
        
        # === B∆Ø·ªöC 2: HYBRID SEARCH CHO T·ª™NG QUERY ===
        all_results = {}  # Dict ƒë·ªÉ lo·∫°i b·ªè tr√πng l·∫∑p (Key = ID)
        
        for query in expanded_queries:
            hybrid_results = hybrid_search(query, n_results=n_results * 2)
            
            # G·ªôp k·∫øt qu·∫£ (gi·ªØ l·∫°i ƒëi·ªÉm cao nh·∫•t n·∫øu tr√πng ID)
            for result in hybrid_results:
                result_id = result['id']
                if result_id not in all_results or result['hybrid_score'] > all_results[result_id]['relevance_score']:
                    all_results[result_id] = {
                        'id': result_id,
                        'metadata': result['metadata'],
                        'document': result['document'],
                        'distance': result.get('distance', 0),
                        'relevance_score': result['hybrid_score'], # Base hybrid score
                        'score_breakdown': result['score_breakdown'],
                        'bm25_score': result['bm25_score'],
                        'vector_score': result['vector_score'],
                        'confidence': 'high' if result['hybrid_score'] > 0.7 else 'medium' if result['hybrid_score'] > 0.5 else 'low'
                    }
        
        # Chuy·ªÉn th√†nh list v√† s·∫Øp x·∫øp s∆° b·ªô
        scored_results = list(all_results.values())
        scored_results.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        # L·∫•y top ·ª©ng vi√™n ƒë·ªÉ Rerank (Rerank t·ªën t√†i nguy√™n n√™n ch·ªâ l√†m tr√™n top ƒë·∫ßu)
        top_candidates = scored_results[:n_results * 2]
        
        # === B∆Ø·ªöC 3: RERANKING ===
        reranked_results = rerank_results(question, top_candidates)
        
        # L·ªçc k·∫øt qu·∫£ v√† gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
        filtered_results = [
            r for r in reranked_results
            if r.get('relevance_score', 0) >= CONFIDENCE_THRESHOLD  # L·ªçc b·ªè k·∫øt qu·∫£ ƒëi·ªÉm qu√° th·∫•p
        ][:n_results]
        
        logger.info(f"Found {len(filtered_results)} relevant results (from {len(scored_results)} total)")
        if filtered_results:
            top = filtered_results[0]
            logger.info(f"Top result: {top['metadata'].get('disease_name')} "
                       f"(score: {top.get('final_score', top.get('relevance_score')):.3f})")
        
        return {
            "success": True,
            "results": filtered_results,
            "total_found": len(filtered_results),
            "total_searched": len(scored_results),
            "search_method": "Hybrid (BM25 + Vector)" if BM25_ENABLED else "Vector Only"
        }
    except Exception as e:
        logger.error(f"Error in search: {str(e)}", exc_info=True)
        return {"success": False, "message": str(e), "results": []}

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# SINH C√ÇU TR·∫¢ L·ªúI T·ª∞ NHI√äN (GENERATION)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def generate_natural_response(
    question: str,
    search_results: List[Dict],
    extracted_features: Dict[str, Any],
    conversation_id: Optional[int] = None,
    user_name: Optional[str] = None,
    image_base64: Optional[str] = None
) -> Dict[str, Any]:
    """
    Sinh c√¢u tr·∫£ l·ªùi t·ª± nhi√™n b·∫±ng GPT-4o, k·∫øt h·ª£p:
    1. Th√¥ng tin t√¨m ki·∫øm ƒë∆∞·ª£c (Context)
    2. L·ªãch s·ª≠ tr√≤ chuy·ªán
    3. H·ªì s∆° s·ª©c kh·ªèe ng∆∞·ªùi d√πng
    4. Kh·∫£ nƒÉng g·ªçi Tool t·ª± ƒë·ªông (Agentic)
    """
    try:
        logger.info(f"Generating response with GPT (User: {user_name})")
        
        # 1. T·∫†O CONTEXT T·ª™ L·ªäCH S·ª¨ CHAT
        conversation_context = ""
        conversation_summary = ""
        
        if conversation_id:
            try:
                from src.models.message import Message
                from src.models.conversation import Conversation
                
                # L·∫•y t√≥m t·∫Øt n·∫øu c√≥
                conversation = Conversation.query.get(conversation_id)
                if conversation and conversation.summary:
                    conversation_summary = conversation.summary
                
                # L·∫•y 5 tin nh·∫Øn g·∫ßn nh·∫•t
                recent_messages = Message.query.filter_by(
                    conversation_id=conversation_id
                ).order_by(Message.sent_at.desc()).limit(5).all()
                
                if recent_messages:
                    recent_messages.reverse()
                    context_parts = []
                    for msg in recent_messages:
                        sender_label = "Ng∆∞·ªùi d√πng" if msg.sender == 'user' else "B√°c sƒ© AI"
                        context_parts.append(f"{sender_label}: {msg.message_text}")
                    
                    conversation_context = "\n".join(context_parts)
            except Exception as e:
                logger.warning(f"Could not load conversation context: {e}")
        
        # 2. T·∫†O CONTEXT T·ª™ H·ªí S∆† S·ª®C KH·ªéE
        health_profile_context = ""
        # (L·∫•y th√¥ng tin t·ª´ DB nh∆∞ng code logic h∆°i ph·ª©c t·∫°p n√™n b·ªè qua vi·ªác query tr·ª±c ti·∫øp ·ªü ƒë√¢y ƒë·ªÉ tr√°nh l·ªói circular import)
        if conversation_id:
             try:
                from src.models.conversation import Conversation
                from src.services.health_profile_service import health_profile_service
                
                conversation = Conversation.query.get(conversation_id)
                if conversation:
                    profile_text = health_profile_service.format_profile_for_chatbot(conversation.user_id)
                    if profile_text:
                        health_profile_context = f"""
„ÄêH·ªí S∆† S·ª®C KH·ªéE C√Å NH√ÇN„Äë
{profile_text}

‚ö†Ô∏è QUAN TR·ªåNG: H√£y tham kh·∫£o h·ªì s∆° n√†y khi t∆∞ v·∫•n. 
- N·∫øu user D·ªä ·ª®NG v·ªõi thu·ªëc/th·ª±c ph·∫©m n√†o ‚Üí TUY·ªÜT ƒê·ªêI KH√îNG ƒë·ªÅ xu·∫•t
- N·∫øu c√≥ b·ªánh m√£n t√≠nh ‚Üí L∆∞u √Ω t∆∞∆°ng t√°c thu·ªëc v√† ch·∫ø ƒë·ªô ƒÉn
"""
             except Exception as e:
                pass


        if not search_results:
            return {
                "answer": """Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong c∆° s·ªü d·ªØ li·ªáu y t·∫ø ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa b·∫°n.

‚ö†Ô∏è Khuy·∫øn c√°o: Vui l√≤ng tham kh·∫£o √Ω ki·∫øn b√°c sƒ© chuy√™n khoa ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c v√† an to√†n.""",
                "sources": [],
                "confidence": "none"
            }
        
        # 3. CHU·∫®N B·ªä CONTEXT T·ª™ K·∫æT QU·∫¢ T√åM KI·∫æM
        context_parts = []
        for idx, result in enumerate(search_results[:3], 1): # L·∫•y top 3 k·∫øt qu·∫£ t·ªët nh·∫•t
            metadata = result['metadata']
            
            # ∆Øu ti√™n d√πng c√¢u tr·∫£ l·ªùi g·ªëc n·∫øu c√≥ (High Quality Data)
            original_answer = metadata.get('original_answer', '')
            original_question = metadata.get('original_question', '')
            
            if original_answer and len(original_answer) > 50:
                context_parts.append(f"""
[Ngu·ªìn {idx}] {metadata.get('source', 'Medical Database')}
C√¢u h·ªèi g·ªëc: {original_question if original_question else metadata.get('disease_name', 'N/A')}
C√¢u tr·∫£ l·ªùi: {original_answer}
ƒê·ªô li√™n quan: {result.get('relevance_score', 0):.2f}
""")
            else:
                # N·∫øu kh√¥ng, d√πng th√¥ng tin c·∫•u tr√∫c
                context_parts.append(f"""
[Ngu·ªìn {idx}] B·ªánh: {metadata.get('disease_name', 'N/A')}
- Tri·ªáu ch·ª©ng: {metadata.get('symptoms', 'N/A')}
- ƒêi·ªÅu tr·ªã: {metadata.get('treatment', 'N/A')}
- Ph√≤ng ng·ª´a: {metadata.get('prevention', 'N/A')}
- ƒê·ªô li√™n quan: {result.get('relevance_score', 0):.2f}
""")
        context = "\n".join(context_parts)
        
        greeting_instruction = f'- B·∫Øt ƒë·∫ßu b·∫±ng "Ch√†o b·∫°n {user_name},"' if user_name else '- B·∫Øt ƒë·∫ßu b·∫±ng "Ch√†o b·∫°n,"'
        
        # 4. SYSTEM PROMPT (K·ªäCH B·∫¢N CH√çNH CHO GPT)
        system_prompt = f"""
B·∫°n l√† B√°c sƒ© AI v·ªõi 10 nƒÉm kinh nghi·ªám l√¢m s√†ng, chuy√™n t∆∞ v·∫•n s·ª©c kh·ªèe cho ng∆∞·ªùi Vi·ªát Nam.

QUY T·∫ÆC B·∫ÆT BU·ªòC (QUAN TR·ªåNG NH·∫§T):
1. ‚úÖ S·ª¨ D·ª§NG CH√çNH X√ÅC th√¥ng tin t·ª´ [Ngu·ªìn] ƒë∆∞·ª£c cung c·∫•p n·∫øu c√≥
2. ‚úÖ N·∫æU C√ì NGU·ªíN: ∆Øu ti√™n tr√≠ch d·∫´n v√† b√°m s√°t n·ªôi dung
3. ‚ö†Ô∏è N·∫æU KH√îNG C√ì NGU·ªíN: ƒê∆∞·ª£c ph√©p s·ª≠ d·ª•ng ki·∫øn th·ª©c y khoa chu·∫©n x√°c ƒë·ªÉ t∆∞ v·∫•n, NH∆ØNG ph·∫£i b·∫Øt ƒë·∫ßu b·∫±ng: "D·ª±a tr√™n ki·∫øn th·ª©c y khoa t·ªïng qu√°t (kh√¥ng c√≥ trong d·ªØ li·ªáu c·ª• th·ªÉ)..."
4. ‚ùå KH√îNG k√™ ƒë∆°n thu·ªëc c·ª• th·ªÉ, ch·ªâ ƒë∆∞a ra l·ªùi khuy√™n v·ªÅ nh√≥m thu·ªëc ho·∫∑c ho·∫°t ch·∫•t
5. ‚ùå KH√îNG ch·∫©n ƒëo√°n kh·∫≥ng ƒë·ªãnh, lu√¥n khuy√™n ng∆∞·ªùi d√πng ƒëi kh√°m b√°c sƒ©
6. ‚úÖ Lu√¥n gi·ªØ th√°i ƒë·ªô kh√°ch quan, khoa h·ªçc v√† c·∫£m th√¥ng

ü§ñ AUTONOMOUS DECISION MAKING (QUAN TR·ªåNG NH·∫§T):
B·∫°n c√≥ quy·ªÅn truy c·∫≠p v√†o c√°c c√¥ng c·ª• (tools) ƒë·ªÉ CH·ª¶ ƒê·ªòNG h·ªó tr·ª£ user:

**Tool 1: lay_thong_tin_nguoi_dung**
- L·∫•y h·ªì s∆° s·ª©c kh·ªèe, l·ªãch u·ªëng thu·ªëc, thu·ªëc s·∫Øp u·ªëng
- ‚úÖ T·ª∞ ƒê·ªòNG G·ªåI khi user n√≥i v·ªÅ tri·ªáu ch·ª©ng (ƒëau ƒë·∫ßu, s·ªët, ho...)
- ‚úÖ T·ª∞ ƒê·ªòNG G·ªåI khi user h·ªèi v·ªÅ thu·ªëc
- ‚úÖ T·ª∞ ƒê·ªòNG G·ªåI ƒë·ªÉ check d·ªã ·ª©ng tr∆∞·ªõc khi ƒë·ªÅ xu·∫•t

**Tool 2: tim_benh_vien_gan_nhat**
- T√¨m b·ªánh vi·ªán g·∫ßn user (c·∫ßn v·ªã tr√≠ GPS)
- G·ªçi khi user c·∫ßn ƒëi kh√°m ho·∫∑c h·ªèi v·ªÅ b·ªánh vi·ªán

{health_profile_context if health_profile_context else ""}

C√ÅCH TR·∫¢ L·ªúI:
{greeting_instruction}
- Tr·∫£ l·ªùi D·ª∞A TR√äN n·ªôi dung t·ª´ [Ngu·ªìn]
- N·∫øu ngu·ªìn c√≥ "C√¢u tr·∫£ l·ªùi" g·ªëc ‚Üí D√πng n·ªôi dung ƒë√≥
- Chia th√†nh 2-3 ƒëo·∫°n ng·∫Øn, d√πng bullet points (‚Ä¢)
- Gi·ªçng ƒëi·ªáu th√¢n thi·ªán, kh√¥ng g√¢y ho·∫£ng lo·∫°n

LU√îN KHUY·∫æN C√ÅO ƒêI KH√ÅM B√ÅC Sƒ® N·∫æU:
‚Ä¢ Tri·ªáu ch·ª©ng k√©o d√†i > 3 ng√†y
‚Ä¢ S·ªët cao > 39¬∞C
‚Ä¢ C√≥ d·∫•u hi·ªáu nguy hi·ªÉm: kh√≥ th·ªü, ƒëau ng·ª±c, co gi·∫≠t
"""
        if image_base64:
             logger.info(f"Image attached. Using Vision capabilities.")
             system_prompt += "\n7. üñºÔ∏è C√ì H√åNH ·∫¢NH: H√£y ph√¢n t√≠ch h√¨nh ·∫£nh ƒë∆∞·ª£c g·ª≠i k√®m v√† ƒë∆∞a ra nh·∫≠n x√©t y t·∫ø s∆° b·ªô. Lu√¥n c·∫£nh b√°o ƒë√¢y ch·ªâ l√† ƒë√°nh gi√° d·ª±a tr√™n h√¨nh ·∫£nh."
        
        
        # 5. USER PROMPT (C√ÇU H·ªéI V√Ä N·ªòI DUNG)
        user_prompt_parts = []
        
        if conversation_id:
             try:
                from src.models.conversation import Conversation
                c = Conversation.query.get(conversation_id)
                if c:
                    user_prompt_parts.append(f"User ID: {c.user_id}")
                    user_prompt_parts.append(f"‚ö†Ô∏è S·ª≠ d·ª•ng user_id n√†y khi g·ªçi tool lay_thong_tin_nguoi_dung")
             except: pass
        
        user_prompt_parts.append(f"C√¢u h·ªèi hi·ªán t·∫°i: {question}")
        
        if conversation_summary:
            user_prompt_parts.append(f"„ÄêT√≥m t·∫Øt cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥„Äë\n{conversation_summary}")

        if conversation_context:
            user_prompt_parts.append(f"„ÄêL·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y„Äë\n{conversation_context}\n\n‚ö†Ô∏è L∆ØU √ù: H√£y tham kh·∫£o l·ªãch s·ª≠ ƒë·ªÉ hi·ªÉu ng·ªØ c·∫£nh.")
        
        user_prompt_parts.append(f"„ÄêTh√¥ng tin y t·∫ø t·ª´ c∆° s·ªü d·ªØ li·ªáu„Äë\n{context}")
        user_prompt_parts.append(f"„ÄêTh√¥ng tin tr√≠ch xu·∫•t„Äë\n{json.dumps(extracted_features, ensure_ascii=False)}")
        user_prompt_parts.append("H√£y tr·∫£ l·ªùi theo ƒë√∫ng quy t·∫Øc.")
        
        user_prompt = "\n\n".join(user_prompt_parts)
        
        # 6. G·ªåI GPT (TOOL CALLING FLOW)
        
        messages = [{"role": "system", "content": system_prompt}]
        
        if image_base64:
             # G·ª≠i c·∫£ Text v√† ·∫¢nh
             user_content = []
             user_content.append({"type": "text", "text": user_prompt})
             user_content.append({
                "type": "image_url",
                "image_url": {
                    "url": image_base64 if image_base64.startswith("data:image") else f"data:image/jpeg;base64,{image_base64}"
                }
             })
             messages.append({"role": "user", "content": user_content})
        else:
             messages.append({"role": "user", "content": user_prompt})

        # G·ªçi GPT L·∫ßn 1
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            tools=AVAILABLE_TOOLS,  # Cung c·∫•p danh s√°ch c√¥ng c·ª•
            tool_choice="auto",
            temperature=0.3,
            max_tokens=800
        )
        
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls
        
        # 7. X·ª¨ L√ù TOOL CALLING
        if tool_calls:
            logger.info(f"üîß GPT triggered {len(tool_calls)} tool call(s)")
            messages.append(response_message)
            
            for tool_call in tool_calls:
                function_name = tool_call.function.name
                logger.info(f"Executing tool: {function_name}")
                
                # Th·ª±c thi tool
                function_response = execute_tool_call(tool_call)
                
                # Th√™m k·∫øt qu·∫£ v√†o h·ªôi tho·∫°i
                messages.append({
                    "tool_call_id": tool_call.id,
                    "role": "tool",
                    "name": function_name,
                    "content": function_response
                })
            
            # G·ªçi GPT L·∫ßn 2 (c√≥ th√¥ng tin t·ª´ tool)
            second_response = client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.3,
                max_tokens=800
            )
            
            answer = second_response.choices[0].message.content
            logger.info("‚úì Tool calling completed, final answer generated")
        else:
            # Kh√¥ng g·ªçi tool -> L·∫•y lu√¥n c√¢u tr·∫£ l·ªùi
            answer = response_message.content
        
        # Th√™m c·∫£nh b√°o an to√†n n·∫øu GPT qu√™n
        if "b√°c sƒ©" not in answer.lower() and "kh√°m" not in answer.lower():
            answer += "\n\n‚ö†Ô∏è L∆∞u √Ω: Th√¥ng tin tr√™n ch·ªâ mang t√≠nh ch·∫•t tham kh·∫£o. Vui l√≤ng tham kh·∫£o √Ω ki·∫øn b√°c sƒ© chuy√™n khoa."
        
        avg_score = np.mean([r.get('relevance_score', 0) for r in search_results[:3]])
        confidence = 'high' if avg_score > 0.7 else 'medium' if avg_score > 0.5 else 'low'
        
        logger.info(f"Response generated successfully (confidence: {confidence})")
        
        return {
            "answer": answer,
            "sources": search_results[:3],
            "confidence": confidence,
            "avg_relevance_score": round(avg_score, 3)
        }
        
    except Exception as e:
        logger.error(f"Error generating response: {str(e)}", exc_info=True)
        return {
            "answer": """Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t. Vui l√≤ng th·ª≠ l·∫°i sau.

‚ö†Ô∏è N·∫øu b·∫°n ƒëang g·∫∑p v·∫•n ƒë·ªÅ s·ª©c kh·ªèe c·∫•p b√°ch, vui l√≤ng li√™n h·ªá b√°c sƒ© ngay l·∫≠p t·ª©c.""",
            "error": str(e),
            "sources": [],
            "confidence": "error"
        }
