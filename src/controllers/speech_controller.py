"""
Speech-to-Text API Controller
==============================
Controller cung c·∫•p c√°c endpoints x·ª≠ l√Ω gi·ªçng n√≥i (Speech-to-Text).
Gi√∫p ·ª©ng d·ª•ng Mobile/Web c√≥ th·ªÉ g·ª≠i file √¢m thanh v√† nh·∫≠n v·ªÅ vƒÉn b·∫£n ho·∫∑c c√¢u tr·∫£ l·ªùi t·ª´ chatbot.

Endpoints:
1. POST /api/speech/transcribe - Ch·ªâ chuy·ªÉn ƒë·ªïi Audio -> Text (d√πng cho t√≠nh nƒÉng nh·∫≠p li·ªáu b·∫±ng gi·ªçng n√≥i).
2. POST /api/speech/chat - Chuy·ªÉn ƒë·ªïi Audio -> Text, sau ƒë√≥ g·ª≠i Text v√†o RAG Pipeline ƒë·ªÉ h·ªèi Chatbot.
"""

from flask import request
from flask_restx import Namespace, Resource, fields
import logging
from werkzeug.datastructures import FileStorage

from src.services.speech_service import speech_service  # Service x·ª≠ l√Ω file audio
from src.services.medical_chatbot_service import (
    extract_user_intent_and_features,
    combined_search_with_filters,
    generate_natural_response
)
from src.services.cached_chatbot_service import cached_search, cached_response  # H·ªó tr·ª£ cache ƒë·ªÉ tƒÉng t·ªëc
from src.utils.auth_middleware import token_required  # B·∫£o m·∫≠t API
from src.models.base import db
from src.models.conversation import Conversation
from src.models.message import Message
from datetime import datetime

# Kh·ªüi t·∫°o logger ƒë·ªÉ ghi nh·∫≠n activity
logger = logging.getLogger(__name__)

# Namespace 'speech' -> URL g·ªëc: /api/speech
speech_ns = Namespace(
    'speech',
    description='Speech-to-Text operations - X·ª≠ l√Ω gi·ªçng n√≥i y t·∫ø'
)

# ============================================================================
# API MODELS (ƒê·ªãnh nghƒ©a Interface cho Swagger API)
# ============================================================================

# Model Response cho API Transcribe
transcribe_response = speech_ns.model('TranscribeResponse', {
    'success': fields.Boolean(description='Tr·∫°ng th√°i request (true/false)'),
    'text': fields.String(description='VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi t·ª´ gi·ªçng n√≥i'),
    'language': fields.String(description='Ng√¥n ng·ªØ ph√°t hi·ªán ƒë∆∞·ª£c (VD: vi, en)'),
    'duration': fields.Float(description='ƒê·ªô d√†i file √¢m thanh (gi√¢y)'),
    'message': fields.String(description='Th√¥ng b√°o chi ti·∫øt')
})

# Model Response cho API Chat Voice
chat_response = speech_ns.model('SpeechChatResponse', {
    'success': fields.Boolean(description='Tr·∫°ng th√°i request'),
    'transcribed_text': fields.String(description='N·ªôi dung ng∆∞·ªùi d√πng n√≥i (ƒë√£ chuy·ªÉn th√†nh ch·ªØ)'),
    'question': fields.String(description='C√¢u h·ªèi (nh∆∞ transcribed_text)'),
    'answer': fields.String(description='C√¢u tr·∫£ l·ªùi t·ª´ B√°c sƒ© AI'),
    'conversation_id': fields.Integer(description='ID cu·ªôc h·ªôi tho·∫°i'),
    'message_id': fields.Integer(description='ID tin nh·∫Øn tr·∫£ l·ªùi'),
    'language': fields.String(description='Ng√¥n ng·ªØ'),
    'duration': fields.Float(description='Th·ªùi l∆∞·ª£ng audio')
})

# Parser x·ª≠ l√Ω file upload (Multipart/form-data)
upload_parser = speech_ns.parser()
upload_parser.add_argument(
    'audio',
    location='files',
    type=FileStorage,
    required=True,
    help='File audio (h·ªó tr·ª£ mp3, wav, m4a, webm, ogg, flac). T·ªëi ƒëa 25MB.'
)
upload_parser.add_argument(
    'language',
    location='form',
    type=str,
    required=False,
    default='vi',
    help='M√£ ng√¥n ng·ªØ mong mu·ªën (vi=Ti·∫øng Vi·ªát). ƒê·ªÉ tr·ªëng ƒë·ªÉ t·ª± ƒë·ªông ph√°t hi·ªán.'
)

# Parser m·ªü r·ªông cho Chat endpoint (c·∫ßn th√™m conversation_id)
chat_parser = upload_parser.copy()
chat_parser.add_argument(
    'conversation_id',
    location='form',
    type=int,
    required=False,
    help='ID cu·ªôc h·ªôi tho·∫°i (n·∫øu mu·ªën chat ti·∫øp trong lu·ªìng c≈©)'
)


# ============================================================================
# API ENDPOINTS
# ============================================================================

@speech_ns.route('/transcribe')
class TranscribeAudio(Resource):
    """
    Endpoint ƒë∆°n gi·∫£n: Audio In -> Text Out.
    Th∆∞·ªùng d√πng khi user b·∫•m n√∫t mic ·ªü √¥ input text.
    """
    
    @speech_ns.expect(upload_parser)
    @speech_ns.response(200, 'Success', transcribe_response)
    @speech_ns.response(400, 'Bad Request - File l·ªói')
    @speech_ns.response(500, 'Server Error')
    def post(self):
        """
        Chuy·ªÉn file audio th√†nh vƒÉn b·∫£n (Transcribe).
        """
        try:
            # 1. Ki·ªÉm tra file upload
            if 'audio' not in request.files:
                return {'success': False, 'message': 'No audio file provided'}, 400
            
            audio_file = request.files['audio']
            language = request.form.get('language', 'vi')
            
            logger.info(f"üé§ Transcribe request received: {audio_file.filename}")
            
            # 2. G·ªçi Service x·ª≠ l√Ω (L∆∞u temp -> G·ªçi Whisper API -> X√≥a temp)
            result = speech_service.process_audio_file(audio_file, language=language)
            
            # 3. Tr·∫£ v·ªÅ k·∫øt qu·∫£
            return {
                'success': True,
                'text': result['text'],
                'language': result['language'],
                'duration': result.get('duration', 0),
                'message': 'Transcription successful'
            }, 200
            
        except ValueError as e:
            # L·ªói do file kh√¥ng h·ª£p l·ªá (sai ƒë·ªãnh d·∫°ng, qu√° l·ªõn...)
            logger.warning(f"Validation error: {e}")
            return {'success': False, 'message': str(e)}, 400
            
        except Exception as e:
            # L·ªói kh√¥ng mong mu·ªën
            logger.error(f"Transcription error: {e}", exc_info=True)
            return {'success': False, 'message': f'Failed to process audio: {str(e)}'}, 500


@speech_ns.route('/chat')
class SpeechToChat(Resource):
    """
    Endpoint n√¢ng cao: Audio In -> Text -> RAG Bot -> Answer Out.
    Gi√∫p t·∫°o tr·∫£i nghi·ªám h·ªôi tho·∫°i b·∫±ng gi·ªçng n√≥i m∆∞·ª£t m√† (Voice interaction).
    """
    
    @speech_ns.expect(chat_parser)
    @speech_ns.response(200, 'Success', chat_response)
    @speech_ns.response(401, 'Unauthorized')
    @speech_ns.doc(security='Bearer')
    @token_required
    def post(self, current_user):
        """
        X·ª≠ l√Ω to√†n b·ªô lu·ªìng Chat b·∫±ng gi·ªçng n√≥i.
        """
        try:
            # --- PHASE 1: INPUT HANDLING (NH·∫¨N D·ªÆ LI·ªÜU) ---
            if 'audio' not in request.files:
                return {'success': False, 'message': 'No audio file provided'}, 400
            
            audio_file = request.files['audio']
            language = request.form.get('language', 'vi')
            conversation_id = request.form.get('conversation_id', type=int)
            
            user_id = current_user['user_id']
            user_name = current_user.get('full_name')
            
            logger.info(f"üó£Ô∏è Voice Chat Request: User {user_id} - File {audio_file.filename}")
            
            # --- PHASE 2: SPEECH-TO-TEXT (CHUY·ªÇN ƒê·ªîI) ---
            transcribe_result = speech_service.process_audio_file(audio_file, language=language)
            transcribed_text = transcribe_result['text']
            
            logger.info(f"üìù Transcribed: {transcribed_text[:100]}...")
            
            # --- PHASE 3: CONVERSATION MANAGEMENT (QU·∫¢N L√ù H·ªòI THO·∫†I) ---
            # T√¨m ho·∫∑c t·∫°o h·ªôi tho·∫°i m·ªõi
            conversation = None
            if conversation_id:
                conversation = Conversation.query.filter_by(
                    conversation_id=conversation_id, user_id=user_id
                ).first()
            
            if not conversation:
                conversation = Conversation(
                    user_id=user_id,
                    started_at=datetime.utcnow(),
                    source_language='vi',
                    title=transcribed_text[:50] + "..."  # D√πng ƒëo·∫°n ƒë·∫ßu c√¢u n√≥i l√†m ti√™u ƒë·ªÅ
                )
                db.session.add(conversation)
                db.session.commit()
            
            # L∆∞u tin nh·∫Øn c·ªßa User v√†o DB
            user_msg = Message(
                conversation_id=conversation.conversation_id,
                sender='user',
                message_text=transcribed_text,
                message_type='voice',  # ƒê√°nh d·∫•u l√† tin nh·∫Øn tho·∫°i
                sent_at=datetime.utcnow()
            )
            db.session.add(user_msg)
            db.session.commit()
            
            # --- PHASE 4: RAG PIPELINE (T√åM KI·∫æM & TR·∫¢ L·ªúI) ---
            
            # 1. Ph√¢n t√≠ch √Ω ƒë·ªãnh (Intent Classification) & Tr√≠ch xu·∫•t th·ª±c th·ªÉ
            extraction_result = extract_user_intent_and_features(transcribed_text)
            extracted_features = extraction_result.get('extracted_features', {})
            
            # 2. T√¨m ki·∫øm th√¥ng tin (Hybrid Search) - C√≥ d√πng Cache
            search_result = cached_search(
                combined_search_with_filters,
                transcribed_text,
                extracted_features
            )
            search_results = search_result.get('results', [])
            
            # 3. Sinh c√¢u tr·∫£ l·ªùi (LLM Generation) - C√≥ d√πng Cache
            # (Truy·ªÅn conversation_id ƒë·ªÉ bot nh·ªõ ng·ªØ c·∫£nh c≈©)
            response = cached_response(
                generate_natural_response,
                transcribed_text,
                search_results,
                extracted_features,
                conversation_id=conversation.conversation_id,
                user_name=user_name
            )
            answer = response.get('answer')
            
            # --- PHASE 5: SAVE & RETURN (L∆ØU V√Ä TR·∫¢ V·ªÄ) ---
            
            # L∆∞u c√¢u tr·∫£ l·ªùi c·ªßa Bot
            bot_msg = Message(
                conversation_id=conversation.conversation_id,
                sender='bot',
                message_text=answer,
                message_type='text',  # Bot tr·∫£ l·ªùi b·∫±ng text (App s·∫Ω TTS n·∫øu c·∫ßn)
                sent_at=datetime.utcnow()
            )
            db.session.add(bot_msg)
            db.session.commit()
            
            return {
                'success': True,
                'transcribed_text': transcribed_text,
                'question': transcribed_text,
                'answer': answer,
                'conversation_id': conversation.conversation_id,
                'message_id': bot_msg.message_id,
                'language': transcribe_result['language'],
                'duration': transcribe_result.get('duration', 0)
            }, 200
            
        except ValueError as e:
            logger.warning(f"Validation error: {e}")
            return {'success': False, 'message': str(e)}, 400
            
        except Exception as e:
            logger.error(f"Speech-to-chat error: {e}", exc_info=True)
            db.session.rollback()
            return {'success': False, 'message': f'Failed to process request: {str(e)}'}, 500


@speech_ns.route('/health')
class SpeechHealthCheck(Resource):
    """
    Endpoint ki·ªÉm tra tr·∫°ng th√°i Speech Service.
    """
    
    @speech_ns.response(200, 'Healthy')
    @speech_ns.response(500, 'Unhealthy')
    def get(self):
        """Ki·ªÉm tra dependency (OpenAI, Whisper lib)."""
        try:
            import whisper  # Ki·ªÉm tra th∆∞ vi·ªán (n·∫øu d√πng local)
            
            return {
                'success': True,
                'service': 'speech-to-text',
                'status': 'healthy',
                'type': 'openai-api', # ƒêang d√πng API
                'model_loaded': True
            }, 200
            
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return {
                'success': False,
                'status': 'unhealthy',
                'error': str(e)
            }, 500
