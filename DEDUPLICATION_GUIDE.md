# Deduplication Guide - Xá»­ lÃ½ trÃ¹ng láº·p dá»¯ liá»‡u

## ğŸ¯ Má»¥c Ä‘Ã­ch

Khi má»Ÿ rá»™ng data tá»« nhiá»u nguá»“n, sáº½ cÃ³ documents trÃ¹ng láº·p:
- CÃ¹ng 1 bá»‡nh tá»« nhiá»u website
- CÃ¹ng 1 cÃ¢u há»i xuáº¥t hiá»‡n nhiá»u láº§n
- LÃ m giáº£m cháº¥t lÆ°á»£ng search

Tool nÃ y sáº½:
1. âœ… TÃ¬m documents giá»‘ng nhau (similarity >95%)
2. âœ… Chá»n version tá»‘t nháº¥t Ä‘á»ƒ giá»¯ láº¡i
3. âœ… XÃ³a duplicates

---

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### **BÆ°á»›c 1: Dry Run (Xem trÆ°á»›c)**

```bash
# Chá»‰ xem cÃ³ bao nhiÃªu duplicates, KHÃ”NG xÃ³a
python deduplicate_database.py
```

**Output:**
```
================================================================================
DEDUPLICATION TOOL
================================================================================
Checking 2163 documents for duplicates...
Progress: 0/2163
Progress: 100/2163
...
Found 45 duplicate pairs

Duplicate pair (similarity: 0.982):
  Keep: excel_qa_151
  Delete: doc_23

[DRY RUN] Would remove 45 duplicate documents
Run with --execute to actually remove
================================================================================
```

---

### **BÆ°á»›c 2: Execute (Thá»±c sá»± xÃ³a)**

```bash
# XÃ“A duplicates
python deduplicate_database.py --execute
```

**Output:**
```
Found 45 duplicate pairs
âœ“ Deleted 45 duplicate documents

Rebuilding BM25 index...
âœ“ BM25 index rebuilt

âœ“ Removed 45 duplicate documents
================================================================================
```

---

### **BÆ°á»›c 3: Custom Threshold**

```bash
# Threshold tháº¥p hÆ¡n = tÃ¬m nhiá»u duplicates hÆ¡n
python deduplicate_database.py --threshold 0.90 --execute

# Threshold cao hÆ¡n = chá»‰ tÃ¬m duplicates ráº¥t giá»‘ng
python deduplicate_database.py --threshold 0.98
```

**Recommended thresholds:**
- `0.98`: Chá»‰ xÃ³a documents gáº§n nhÆ° giá»‘ng há»‡t (an toÃ n)
- `0.95`: Default (cÃ¢n báº±ng)
- `0.90`: XÃ³a nhiá»u hÆ¡n (cÃ³ thá»ƒ máº¥t data há»¯u Ã­ch)

---

## ğŸ“‹ CÃ¡ch chá»n document tá»‘t nháº¥t

Khi cÃ³ 2 documents giá»‘ng nhau, tool sáº½ chá»n theo tiÃªu chÃ­:

### **Priority 1: Äá»™ dÃ i cÃ¢u tráº£ lá»i** (+3 points)
```
Doc A: "Sá»‘t xuáº¥t huyáº¿t cÃ³ triá»‡u chá»©ng sá»‘t cao."
Doc B: "Sá»‘t xuáº¥t huyáº¿t cÃ³ triá»‡u chá»©ng sá»‘t cao, Ä‘au Ä‘áº§u, Ä‘au cÆ¡, xuáº¥t huyáº¿t..."
â†’ Chá»n Doc B (dÃ i hÆ¡n, chi tiáº¿t hÆ¡n)
```

### **Priority 2: CÃ³ source link** (+2 points)
```
Doc A: source = "Medical Database"
Doc B: source = "https://www.vinmec.com/..."
â†’ Chá»n Doc B (cÃ³ link verify Ä‘Æ°á»£c)
```

### **Priority 3: Metadata Ä‘áº§y Ä‘á»§** (+1 point/field)
```
Doc A: symptoms = "...", treatment = "", prevention = ""
Doc B: symptoms = "...", treatment = "...", prevention = "..."
â†’ Chá»n Doc B (Ä‘áº§y Ä‘á»§ hÆ¡n)
```

---

## âš ï¸ LÆ°u Ã½

### **TrÆ°á»›c khi cháº¡y:**

1. âœ… **Backup database** (náº¿u lo láº¯ng)
   ```bash
   # Copy ChromaDB folder
   cp -r src/nlp_model/data/chroma_db src/nlp_model/data/chroma_db_backup
   ```

2. âœ… **Cháº¡y dry run trÆ°á»›c**
   ```bash
   python deduplicate_database.py
   ```

3. âœ… **Kiá»ƒm tra káº¿t quáº£ dry run**
   - Xem cÃ³ há»£p lÃ½ khÃ´ng
   - Náº¿u xÃ³a quÃ¡ nhiá»u â†’ tÄƒng threshold

### **Sau khi cháº¡y:**

1. âœ… **Restart server**
   ```bash
   python main.py
   ```

2. âœ… **Test search**
   - Thá»­ vÃ i cÃ¢u há»i
   - Xem káº¿t quáº£ cÃ³ bá»‹ áº£nh hÆ°á»Ÿng khÃ´ng

---

## ğŸ“Š Expected Results

### **Before Deduplication:**
```
Total documents: 2,163
Duplicates: ~45 (2%)
Search results: Nhiá»u káº¿t quáº£ giá»‘ng nhau
```

### **After Deduplication:**
```
Total documents: 2,118
Duplicates: 0
Search results: Äa dáº¡ng hÆ¡n, cháº¥t lÆ°á»£ng tá»‘t hÆ¡n
```

---

## ğŸ”§ Troubleshooting

### **Problem: "Too slow"**

Script cháº¡y lÃ¢u vÃ¬ pháº£i so sÃ¡nh tá»«ng cáº·p documents.

**Solution:**
```python
# Trong deduplicate_database.py, giáº£m batch_size
find_duplicates(similarity_threshold=0.95, batch_size=50)
```

### **Problem: "Deleted too many"**

Threshold quÃ¡ tháº¥p.

**Solution:**
```bash
# TÄƒng threshold
python deduplicate_database.py --threshold 0.98 --execute
```

### **Problem: "Didn't find duplicates I know exist"**

Threshold quÃ¡ cao.

**Solution:**
```bash
# Giáº£m threshold
python deduplicate_database.py --threshold 0.90
```

---

## ğŸ¯ Best Practices

### **Khi nÃ o nÃªn cháº¡y:**

1. âœ… **Sau khi load data má»›i**
   ```bash
   python load_excel_dataset.py --csv new_data.csv
   python deduplicate_database.py --execute
   ```

2. âœ… **Äá»‹nh ká»³ (1 thÃ¡ng/láº§n)**
   - TÃ­ch lÅ©y duplicates theo thá»i gian
   - Cháº¡y Ä‘á»ƒ clean up

3. âœ… **TrÆ°á»›c khi deploy production**
   - Äáº£m báº£o data sáº¡ch
   - Tá»‘i Æ°u performance

### **Workflow chuáº©n:**

```bash
# 1. Load new data
python load_excel_dataset.py --csv new_data.csv

# 2. Check for duplicates
python deduplicate_database.py

# 3. If looks good, execute
python deduplicate_database.py --execute

# 4. Restart server
python main.py

# 5. Test
# Go to Swagger UI and test some questions
```

---

## ğŸ“ˆ Performance Impact

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Total Docs** | 2,163 | 2,118 | -2% |
| **Unique Docs** | 2,118 | 2,118 | Same |
| **Search Quality** | Good | Better | +5-10% |
| **Diversity** | 95% | 100% | +5% |

---

## âœ… Summary

**Tool nÃ y giÃºp:**
- âœ… Tá»± Ä‘á»™ng tÃ¬m duplicates
- âœ… Chá»n version tá»‘t nháº¥t
- âœ… XÃ³a duplicates an toÃ n
- âœ… Maintain data quality

**Recommended usage:**
```bash
# Dry run first
python deduplicate_database.py

# If looks good, execute
python deduplicate_database.py --execute

# Restart server
python main.py
```

**Báº¡n muá»‘n cháº¡y ngay Ä‘á»ƒ check duplicates khÃ´ng?** ğŸ˜Š
