# ğŸ“Š PhoBERT Medical Chatbot Evaluation

Há»‡ thá»‘ng Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh Medical Chatbot sá»­ dá»¥ng PhoBERT RAG.

## ğŸ“ Cáº¥u TrÃºc ThÆ° Má»¥c

```
evaluation/
â”œâ”€â”€ evaluate_model.py                          # Script Python cháº¡y local
â”œâ”€â”€ phobert_medical_chatbot_evaluation.ipynb  # Jupyter Notebook (cho Colab)
â”œâ”€â”€ README.md                                  # File nÃ y
â””â”€â”€ test_data/
    â””â”€â”€ sample_test_questions.csv             # Sample test data
```

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng (Local - Windows)

### **BÆ°á»›c 1: CÃ i Äáº·t Dependencies**

```bash
# Äáº£m báº£o Ä‘Ã£ activate virtual environment
cd d:\ChatbotMedical_server\ChatbotMedical_server

# CÃ i Ä‘áº·t thÃªm packages cho evaluation (náº¿u chÆ°a cÃ³)
pip install tqdm pandas numpy
```

### **BÆ°á»›c 2: Chuáº©n Bá»‹ Test Data**

Táº¡o file CSV vá»›i format:

```csv
question,expected_answer,relevant_doc_ids,category
"Sá»‘t xuáº¥t huyáº¿t cÃ³ triá»‡u chá»©ng gÃ¬?","Sá»‘t cao, Ä‘au Ä‘áº§u...","doc_123,doc_456","symptoms"
```

Hoáº·c dÃ¹ng sample data cÃ³ sáºµn: `test_data/sample_test_questions.csv`

### **BÆ°á»›c 3: Cháº¡y Evaluation**

```bash
# Cháº¡y vá»›i sample data
python evaluation/evaluate_model.py --test_file evaluation/test_data/sample_test_questions.csv

# Cháº¡y vá»›i file tÃ¹y chá»‰nh
python evaluation/evaluate_model.py --test_file path/to/your/test.csv --output results.csv

# Cháº¡y vá»›i K values khÃ¡c
python evaluation/evaluate_model.py --test_file test.csv --k_values 1 3 5 10
```

### **BÆ°á»›c 4: Xem Káº¿t Quáº£**

Káº¿t quáº£ sáº½ hiá»ƒn thá»‹ trÃªn console:

```
============================================================
ğŸ“Š EVALUATION RESULTS
============================================================

ğŸ” RETRIEVAL METRICS:
   Precision@1: 0.8000
   Precision@3: 0.7333
   Recall@3: 0.9000
   MRR: 0.8500

ğŸ’¬ RESPONSE QUALITY METRICS:
   Semantic Similarity: 0.7823
   Entity Accuracy: 0.8500

âš¡ PERFORMANCE METRICS:
   Avg Response Time: 2.34s

ğŸ¯ INTERPRETATION:
   âœ… Retrieval: GOOD (Precision@3 â‰¥ 0.7)
   âœ… Response Quality: GOOD (Semantic Similarity â‰¥ 0.7)
   âœ… Medical Accuracy: GOOD (Entity Accuracy â‰¥ 0.8)

ğŸ“ˆ OVERALL SCORE: 3/3
   ğŸ‰ Model is GOOD - Ready for production!
============================================================
```

Files Ä‘Æ°á»£c táº¡o:
- `evaluation_results.csv`: Káº¿t quáº£ chi tiáº¿t tá»«ng cÃ¢u há»i
- `evaluation_results_summary.json`: Metrics tá»•ng há»£p

---

## ğŸ“Š Metrics Giáº£i ThÃ­ch

### **Retrieval Metrics** (ÄÃ¡nh giÃ¡ kháº£ nÄƒng tÃ¬m kiáº¿m)

| Metric | Ã NghÄ©a | Tá»‘t |
|--------|---------|-----|
| **Precision@K** | Tá»· lá»‡ tÃ i liá»‡u Ä‘Ãºng trong top K | â‰¥ 0.7 |
| **Recall@K** | Tá»· lá»‡ tÃ¬m Ä‘Æ°á»£c tÃ i liá»‡u Ä‘Ãºng | â‰¥ 0.8 |
| **MRR** | Vá»‹ trÃ­ cá»§a káº¿t quáº£ Ä‘Ãºng Ä‘áº§u tiÃªn | â‰¥ 0.8 |
| **NDCG@K** | Cháº¥t lÆ°á»£ng ranking tá»•ng thá»ƒ | â‰¥ 0.8 |

### **Response Quality Metrics** (Cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i)

| Metric | Ã NghÄ©a | Tá»‘t |
|--------|---------|-----|
| **Semantic Similarity** | Äá»™ giá»‘ng vá» Ã½ nghÄ©a (PhoBERT) | â‰¥ 0.7 |
| **Entity Accuracy** | Tá»· lá»‡ thuáº­t ngá»¯ y táº¿ Ä‘Ãºng | â‰¥ 0.8 |

### **Performance Metrics**

| Metric | Ã NghÄ©a | Tá»‘t |
|--------|---------|-----|
| **Response Time** | Thá»i gian pháº£n há»“i | < 3s |

---

## ğŸ¯ ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh

### âœ… **MÃ´ HÃ¬nh Tá»T** (Score â‰¥ 2/3)

Äáº¡t Ã­t nháº¥t 2 trong 3 tiÃªu chÃ­:
- âœ… Precision@3 â‰¥ 0.7
- âœ… Semantic Similarity â‰¥ 0.7
- âœ… Entity Accuracy â‰¥ 0.8

â†’ **CÃ³ thá»ƒ deploy production!**

### âš ï¸ **MÃ´ HÃ¬nh TRUNG BÃŒNH** (Score = 1/3)

Chá»‰ Ä‘áº¡t 1 tiÃªu chÃ­ â†’ **Cáº§n cáº£i thiá»‡n**

**Náº¿u Retrieval kÃ©m:**
- Fine-tune PhoBERT
- TÄƒng BM25 weight
- ThÃªm query expansion

**Náº¿u Response Quality kÃ©m:**
- Cáº£i thiá»‡n prompt GPT
- TÄƒng sá»‘ context documents
- Giáº£m temperature

### âŒ **MÃ´ HÃ¬nh Yáº¾U** (Score = 0/3)

KhÃ´ng Ä‘áº¡t tiÃªu chÃ­ nÃ o â†’ **Cáº§n cáº£i thiá»‡n lá»›n**

---

## ğŸ”§ TÃ¹y Chá»‰nh

### **1. Thay Äá»•i K Values**

```bash
python evaluation/evaluate_model.py --test_file test.csv --k_values 1 3 5 10
```

### **2. Chá»‰ Test Retrieval (KhÃ´ng Cáº§n OpenAI API)**

Má»Ÿ file `evaluate_model.py`, comment dÃ²ng generation:

```python
# Line ~120
try:
    generated_answer = generate_natural_response(...)
except:
    generated_answer = "[Generation skipped]"  # â† Sáº½ skip generation
```

### **3. ThÃªm Medical Keywords**

Má»Ÿ file `evaluate_model.py`, sá»­a hÃ m `extract_medical_entities`:

```python
medical_keywords = [
    'sá»‘t', 'Ä‘au', 'viÃªm', 'nhiá»…m', 'bá»‡nh', 'thuá»‘c',
    # ThÃªm keywords cá»§a báº¡n
    'tiá»ƒu Ä‘Æ°á»ng', 'huyáº¿t Ã¡p', 'cholesterol'
]
```

---

## â“ FAQ

### **Q: Script bÃ¡o lá»—i "No module named 'tqdm'"?**
**A:** CÃ i Ä‘áº·t: `pip install tqdm pandas numpy`

### **Q: Script cháº¡y lÃ¢u quÃ¡?**
**A:** 
- Giáº£m sá»‘ test questions
- Comment generation (chá»‰ test retrieval)
- Sá»­ dá»¥ng GPU náº¿u cÃ³

### **Q: KhÃ´ng cÃ³ OpenAI API key?**
**A:** Script váº«n cháº¡y Ä‘Æ°á»£c, chá»‰ skip pháº§n generation. Metrics retrieval váº«n tÃ­nh Ä‘Æ°á»£c.

### **Q: LÃ m sao láº¥y `relevant_doc_ids`?**
**A:**
1. Cháº¡y retrieval cho cÃ¢u há»i
2. Xem top 5 results
3. Chá»n doc IDs Ä‘Ãºng
4. Ghi vÃ o CSV

### **Q: Káº¿t quáº£ khÃ¡c vá»›i Colab?**
**A:** CÃ³ thá»ƒ do:
- PhiÃªn báº£n thÆ° viá»‡n khÃ¡c nhau
- CPU vs GPU (embeddings cÃ³ thá»ƒ khÃ¡c nhau chÃºt)
- Random seed trong GPT

---

## ğŸ“ Há»— Trá»£

Náº¿u gáº·p lá»—i, kiá»ƒm tra:

1. âœ… Virtual environment Ä‘Ã£ activate
2. âœ… ChromaDB cÃ³ dá»¯ liá»‡u (`collection.count() > 0`)
3. âœ… Test file format Ä‘Ãºng (CSV/JSON)
4. âœ… `.env` cÃ³ `OPENAI_API_KEY` (náº¿u test generation)

---

**Good luck! ğŸš€**
