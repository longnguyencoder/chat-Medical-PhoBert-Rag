{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üè• ƒê√°nh Gi√° M√¥ H√¨nh Medical Chatbot - PhoBERT RAG\n",
    "\n",
    "**Notebook n√†y ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa h·ªá th·ªëng Medical Chatbot s·ª≠ d·ª•ng:**\n",
    "- **PhoBERT** (vinai/phobert-base) cho embedding ti·∫øng Vi·ªát\n",
    "- **Hybrid Search** (BM25 + Vector Search)\n",
    "- **RAG** (Retrieval-Augmented Generation) v·ªõi GPT-4\n",
    "\n",
    "## üìä C√°c Metrics ƒê√°nh Gi√°\n",
    "\n",
    "### 1. **Retrieval Metrics** (ƒê√°nh gi√° kh·∫£ nƒÉng t√¨m ki·∫øm)\n",
    "- **Precision@K**: T·ª∑ l·ªá k·∫øt qu·∫£ ƒë√∫ng trong top K k·∫øt qu·∫£\n",
    "- **Recall@K**: T·ª∑ l·ªá t√¨m ƒë∆∞·ª£c t√†i li·ªáu ƒë√∫ng trong top K\n",
    "- **MRR (Mean Reciprocal Rank)**: V·ªã tr√≠ trung b√¨nh c·ªßa k·∫øt qu·∫£ ƒë√∫ng ƒë·∫ßu ti√™n\n",
    "- **NDCG@K**: Normalized Discounted Cumulative Gain\n",
    "\n",
    "### 2. **Response Quality Metrics** (ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi)\n",
    "- **BLEU Score**: So s√°nh n-gram v·ªõi c√¢u tr·∫£ l·ªùi chu·∫©n\n",
    "- **ROUGE Score**: ƒêo ƒë·ªô ch·ªìng l·∫•p v·ªõi c√¢u tr·∫£ l·ªùi chu·∫©n\n",
    "- **BERTScore**: ƒêo semantic similarity b·∫±ng BERT\n",
    "- **Semantic Similarity**: Cosine similarity gi·ªØa embeddings\n",
    "\n",
    "### 3. **Medical Accuracy Metrics** (ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c y t·∫ø)\n",
    "- **Medical Entity Accuracy**: T·ª∑ l·ªá th·ª±c th·ªÉ y t·∫ø (b·ªánh, thu·ªëc) ƒë√∫ng\n",
    "- **Factual Consistency**: ƒê·ªô nh·∫•t qu√°n v·ªõi ngu·ªìn\n",
    "- **Hallucination Rate**: T·ª∑ l·ªá th√¥ng tin sai/kh√¥ng c√≥ trong ngu·ªìn\n",
    "\n",
    "### 4. **System Performance**\n",
    "- **Response Time**: Th·ªùi gian ph·∫£n h·ªìi\n",
    "- **Token Usage**: S·ªë token GPT s·ª≠ d·ª•ng\n",
    "- **Cost Estimation**: ∆Ø·ªõc t√≠nh chi ph√≠ API\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng\n",
    "\n",
    "1. **Upload test dataset** (CSV/JSON) v·ªõi format:\n",
    "   ```json\n",
    "   {\n",
    "     \"question\": \"S·ªët xu·∫•t huy·∫øt c√≥ tri·ªáu ch·ª©ng g√¨?\",\n",
    "     \"expected_answer\": \"S·ªët cao, ƒëau ƒë·∫ßu, ƒëau kh·ªõp...\",\n",
    "     \"relevant_doc_ids\": [\"doc_123\", \"doc_456\"]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **C·∫•u h√¨nh API keys** trong ph·∫ßn Setup\n",
    "\n",
    "3. **Ch·∫°y t·ª´ng cell** ƒë·ªÉ ƒë√°nh gi√°\n",
    "\n",
    "4. **Xem k·∫øt qu·∫£** ·ªü cu·ªëi notebook (charts + summary table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üì¶ 1. C√†i ƒê·∫∑t Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers torch chromadb openai python-dotenv\n",
    "!pip install -q nltk rouge-score bert-score sacrebleu\n",
    "!pip install -q scikit-learn pandas matplotlib seaborn tqdm\n",
    "!pip install -q sentence-transformers  # For BERTScore and reranking\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## üìö 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports_code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# NLP Libraries\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from openai import OpenAI\n",
    "\n",
    "# Evaluation Metrics\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## ‚öôÔ∏è 3. Configuration & API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_code"
   },
   "outputs": [],
   "source": [
    "# ==================== C·∫§U H√åNH API ====================\n",
    "# Nh·∫≠p API keys c·ªßa b·∫°n\n",
    "OPENAI_API_KEY = \"your-openai-api-key-here\"  # Thay b·∫±ng key th·∫≠t\n",
    "\n",
    "# Ho·∫∑c upload file .env v√† uncomment d√≤ng d∆∞·ªõi:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload file .env\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ==================== C·∫§U H√åNH M√î H√åNH ====================\n",
    "PHOBERT_MODEL = \"vinai/phobert-base\"  # Gi·ªëng v·ªõi backend c·ªßa b·∫°n\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"üîß Device: {DEVICE}\")\n",
    "print(f\"ü§ñ PhoBERT Model: {PHOBERT_MODEL}\")\n",
    "print(f\"‚úÖ Configuration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phobert_class"
   },
   "source": [
    "## üß† 4. PhoBERT Embedding Class (Gi·ªëng Backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phobert_code"
   },
   "outputs": [],
   "source": [
    "class PhoBERTEmbedding:\n",
    "    \"\"\"PhoBERT embedding function - gi·ªëng v·ªõi backend\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=PHOBERT_MODEL, device=DEVICE, max_length=256):\n",
    "        print(f\"Loading PhoBERT model: {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        print(f\"‚úÖ PhoBERT loaded on {device}\")\n",
    "    \n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for texts\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        encoded = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        encoded = {k: v.to(self.device) for k, v in encoded.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model(**encoded)\n",
    "        \n",
    "        # Mean pooling\n",
    "        embeddings = self._mean_pooling(output.last_hidden_state, encoded['attention_mask'])\n",
    "        return embeddings.cpu().numpy()\n",
    "    \n",
    "    def _mean_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "# Initialize PhoBERT\n",
    "phobert = PhoBERTEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_data"
   },
   "source": [
    "## üìÇ 5. Load Test Dataset\n",
    "\n",
    "**Upload file CSV ho·∫∑c JSON v·ªõi format:**\n",
    "\n",
    "```csv\n",
    "question,expected_answer,relevant_doc_ids\n",
    "\"S·ªët xu·∫•t huy·∫øt c√≥ tri·ªáu ch·ª©ng g√¨?\",\"S·ªët cao ƒë·ªôt ng·ªôt, ƒëau ƒë·∫ßu, ƒëau kh·ªõp, xu·∫•t huy·∫øt\",\"doc_123,doc_456\"\n",
    "```\n",
    "\n",
    "Ho·∫∑c t·∫°o test data m·∫´u:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_data_code"
   },
   "outputs": [],
   "source": [
    "# ==================== OPTION 1: Upload File ====================\n",
    "# Uncomment ƒë·ªÉ upload file t·ª´ m√°y t√≠nh\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# test_data_file = list(uploaded.keys())[0]\n",
    "# test_df = pd.read_csv(test_data_file) if test_data_file.endswith('.csv') else pd.read_json(test_data_file)\n",
    "\n",
    "# ==================== OPTION 2: Sample Data ====================\n",
    "# T·∫°o test data m·∫´u (thay b·∫±ng data th·∫≠t c·ªßa b·∫°n)\n",
    "test_data = [\n",
    "    {\n",
    "        \"question\": \"S·ªët xu·∫•t huy·∫øt c√≥ tri·ªáu ch·ª©ng g√¨?\",\n",
    "        \"expected_answer\": \"S·ªët cao ƒë·ªôt ng·ªôt (39-40¬∞C), ƒëau ƒë·∫ßu d·ªØ d·ªôi, ƒëau m·ªèi c∆° v√† kh·ªõp, bu·ªìn n√¥n, n√¥n, xu·∫•t hi·ªán ban xu·∫•t huy·∫øt d∆∞·ªõi da.\",\n",
    "        \"relevant_doc_ids\": [\"dengue_001\"],\n",
    "        \"category\": \"symptoms\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"C√°ch ƒëi·ªÅu tr·ªã s·ªët xu·∫•t huy·∫øt?\",\n",
    "        \"expected_answer\": \"Ngh·ªâ ng∆°i tuy·ªát ƒë·ªëi, u·ªëng nhi·ªÅu n∆∞·ªõc, h·∫° s·ªët b·∫±ng paracetamol, theo d√µi s·ªë l∆∞·ª£ng ti·ªÉu c·∫ßu. Kh√¥ng d√πng aspirin hay ibuprofen. Nh·∫≠p vi·ªán n·∫øu xu·∫•t huy·∫øt n·∫∑ng.\",\n",
    "        \"relevant_doc_ids\": [\"dengue_001\", \"dengue_treatment_001\"],\n",
    "        \"category\": \"treatment\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Vi√™m gan B l√¢y qua ƒë∆∞·ªùng n√†o?\",\n",
    "        \"expected_answer\": \"L√¢y qua ƒë∆∞·ªùng m√°u (kim ti√™m, d·ª•ng c·ª• y t·∫ø), quan h·ªá t√¨nh d·ª•c kh√¥ng an to√†n, t·ª´ m·∫π sang con khi sinh.\",\n",
    "        \"relevant_doc_ids\": [\"hepatitis_b_001\"],\n",
    "        \"category\": \"transmission\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Paracetamol u·ªëng li·ªÅu bao nhi√™u?\",\n",
    "        \"expected_answer\": \"Ng∆∞·ªùi l·ªõn: 500-1000mg m·ªói l·∫ßn, c√°ch 4-6 gi·ªù, t·ªëi ƒëa 4000mg/ng√†y. Tr·∫ª em: 10-15mg/kg c√¢n n·∫∑ng m·ªói l·∫ßn.\",\n",
    "        \"relevant_doc_ids\": [\"paracetamol_001\"],\n",
    "        \"category\": \"medication\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Ph√≤ng ng·ª´a COVID-19 nh∆∞ th·∫ø n√†o?\",\n",
    "        \"expected_answer\": \"Ti√™m v·∫Øc-xin ƒë·∫ßy ƒë·ªß, ƒëeo kh·∫©u trang n∆°i ƒë√¥ng ng∆∞·ªùi, r·ª≠a tay th∆∞·ªùng xuy√™n, gi·ªØ kho·∫£ng c√°ch an to√†n, tr√°nh t·ª• t·∫≠p ƒë√¥ng ng∆∞·ªùi.\",\n",
    "        \"relevant_doc_ids\": [\"covid19_prevention_001\"],\n",
    "        \"category\": \"prevention\"\n",
    "    }\n",
    "]\n",
    "\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(test_df)} test questions\")\n",
    "print(f\"\\nCategories: {test_df['category'].value_counts().to_dict()}\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mock_system"
   },
   "source": [
    "## üîß 6. Mock RAG System (Simulating Your Backend)\n",
    "\n",
    "**L∆∞u √Ω:** ƒê√¢y l√† mock system ƒë·ªÉ test. Trong th·ª±c t·∫ø, b·∫°n n√™n:\n",
    "1. Connect tr·ª±c ti·∫øp ƒë·∫øn backend API c·ªßa b·∫°n\n",
    "2. Ho·∫∑c load ChromaDB t·ª´ backend ƒë·ªÉ test offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mock_system_code"
   },
   "outputs": [],
   "source": [
    "class MockMedicalRAG:\n",
    "    \"\"\"Mock RAG system - thay b·∫±ng API call th·∫≠t ho·∫∑c load ChromaDB\"\"\"\n",
    "    \n",
    "    def __init__(self, phobert_model):\n",
    "        self.phobert = phobert_model\n",
    "        self.documents = self._load_mock_documents()\n",
    "        self.doc_embeddings = self._create_embeddings()\n",
    "    \n",
    "    def _load_mock_documents(self) -> List[Dict]:\n",
    "        \"\"\"Load mock medical documents (thay b·∫±ng ChromaDB th·∫≠t)\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"id\": \"dengue_001\",\n",
    "                \"disease_name\": \"S·ªët xu·∫•t huy·∫øt\",\n",
    "                \"symptoms\": \"S·ªët cao ƒë·ªôt ng·ªôt 39-40¬∞C, ƒëau ƒë·∫ßu d·ªØ d·ªôi, ƒëau m·ªèi c∆° kh·ªõp, bu·ªìn n√¥n, ban xu·∫•t huy·∫øt\",\n",
    "                \"treatment\": \"Ngh·ªâ ng∆°i, u·ªëng nhi·ªÅu n∆∞·ªõc, paracetamol h·∫° s·ªët, theo d√µi ti·ªÉu c·∫ßu\",\n",
    "                \"prevention\": \"Di·ªát mu·ªói, kh√¥ng ƒë·ªÉ n∆∞·ªõc ƒë·ªçng\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"dengue_treatment_001\",\n",
    "                \"disease_name\": \"ƒêi·ªÅu tr·ªã s·ªët xu·∫•t huy·∫øt\",\n",
    "                \"treatment\": \"Kh√¥ng d√πng aspirin, ibuprofen. Ch·ªâ d√πng paracetamol. Nh·∫≠p vi·ªán n·∫øu xu·∫•t huy·∫øt n·∫∑ng, ti·ªÉu c·∫ßu d∆∞·ªõi 50,000\",\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"hepatitis_b_001\",\n",
    "                \"disease_name\": \"Vi√™m gan B\",\n",
    "                \"symptoms\": \"M·ªát m·ªèi, v√†ng da, n∆∞·ªõc ti·ªÉu s·∫´m m√†u, ƒëau b·ª•ng\",\n",
    "                \"transmission\": \"L√¢y qua m√°u, quan h·ªá t√¨nh d·ª•c, m·∫π sang con\",\n",
    "                \"prevention\": \"Ti√™m v·∫Øc-xin, kh√¥ng d√πng chung kim ti√™m\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"paracetamol_001\",\n",
    "                \"medication_name\": \"Paracetamol\",\n",
    "                \"dosage\": \"Ng∆∞·ªùi l·ªõn: 500-1000mg/l·∫ßn, 4-6h/l·∫ßn, max 4000mg/ng√†y. Tr·∫ª em: 10-15mg/kg\",\n",
    "                \"indication\": \"H·∫° s·ªët, gi·∫£m ƒëau\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"covid19_prevention_001\",\n",
    "                \"disease_name\": \"COVID-19\",\n",
    "                \"prevention\": \"Ti√™m v·∫Øc-xin, ƒëeo kh·∫©u trang, r·ª≠a tay, gi·ªØ kho·∫£ng c√°ch\",\n",
    "                \"symptoms\": \"S·ªët, ho, kh√≥ th·ªü, m·∫•t v·ªã gi√°c\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def _create_embeddings(self) -> np.ndarray:\n",
    "        \"\"\"Create embeddings for all documents\"\"\"\n",
    "        texts = []\n",
    "        for doc in self.documents:\n",
    "            # Combine all fields into searchable text\n",
    "            text = \" \".join([str(v) for v in doc.values() if v and isinstance(v, str)])\n",
    "            texts.append(text)\n",
    "        \n",
    "        return self.phobert.encode(texts)\n",
    "    \n",
    "    def search(self, question: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Search for relevant documents\"\"\"\n",
    "        # Encode question\n",
    "        query_embedding = self.phobert.encode([question])\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarities = cosine_similarity(query_embedding, self.doc_embeddings)[0]\n",
    "        \n",
    "        # Get top K\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                \"id\": self.documents[idx][\"id\"],\n",
    "                \"metadata\": self.documents[idx],\n",
    "                \"relevance_score\": float(similarities[idx]),\n",
    "                \"distance\": 1 - float(similarities[idx])\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_answer(self, question: str, search_results: List[Dict]) -> str:\n",
    "        \"\"\"Generate answer using GPT (gi·ªëng backend)\"\"\"\n",
    "        if not search_results:\n",
    "            return \"Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p.\"\n",
    "        \n",
    "        # Prepare context\n",
    "        context_parts = []\n",
    "        for idx, result in enumerate(search_results[:3], 1):\n",
    "            metadata = result['metadata']\n",
    "            context_parts.append(f\"[Ngu·ªìn {idx}] {json.dumps(metadata, ensure_ascii=False)}\")\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Generate with GPT\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"B·∫°n l√† b√°c sƒ© AI. Tr·∫£ l·ªùi d·ª±a tr√™n ngu·ªìn ƒë∆∞·ª£c cung c·∫•p. Kh√¥ng t·ª± suy lu·∫≠n.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Ngu·ªìn:\\n{context}\\n\\nC√¢u h·ªèi: {question}\\n\\nTr·∫£ l·ªùi:\"}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=300\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "# Initialize mock RAG system\n",
    "rag_system = MockMedicalRAG(phobert)\n",
    "print(f\"‚úÖ Mock RAG system initialized with {len(rag_system.documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_functions"
   },
   "source": [
    "## üìä 7. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eval_functions_code"
   },
   "outputs": [],
   "source": [
    "# ==================== RETRIEVAL METRICS ====================\n",
    "\n",
    "def calculate_precision_at_k(retrieved_ids: List[str], relevant_ids: List[str], k: int) -> float:\n",
    "    \"\"\"Precision@K: T·ª∑ l·ªá t√†i li·ªáu ƒë√∫ng trong top K\"\"\"\n",
    "    retrieved_k = retrieved_ids[:k]\n",
    "    relevant_set = set(relevant_ids)\n",
    "    correct = sum(1 for doc_id in retrieved_k if doc_id in relevant_set)\n",
    "    return correct / k if k > 0 else 0.0\n",
    "\n",
    "def calculate_recall_at_k(retrieved_ids: List[str], relevant_ids: List[str], k: int) -> float:\n",
    "    \"\"\"Recall@K: T·ª∑ l·ªá t√¨m ƒë∆∞·ª£c t√†i li·ªáu ƒë√∫ng trong top K\"\"\"\n",
    "    retrieved_k = set(retrieved_ids[:k])\n",
    "    relevant_set = set(relevant_ids)\n",
    "    if len(relevant_set) == 0:\n",
    "        return 0.0\n",
    "    correct = len(retrieved_k & relevant_set)\n",
    "    return correct / len(relevant_set)\n",
    "\n",
    "def calculate_mrr(retrieved_ids: List[str], relevant_ids: List[str]) -> float:\n",
    "    \"\"\"Mean Reciprocal Rank: 1 / v·ªã tr√≠ c·ªßa k·∫øt qu·∫£ ƒë√∫ng ƒë·∫ßu ti√™n\"\"\"\n",
    "    relevant_set = set(relevant_ids)\n",
    "    for i, doc_id in enumerate(retrieved_ids, 1):\n",
    "        if doc_id in relevant_set:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def calculate_ndcg_at_k(retrieved_ids: List[str], relevant_ids: List[str], k: int) -> float:\n",
    "    \"\"\"NDCG@K: Normalized Discounted Cumulative Gain\"\"\"\n",
    "    retrieved_k = retrieved_ids[:k]\n",
    "    relevant_set = set(relevant_ids)\n",
    "    \n",
    "    # DCG\n",
    "    dcg = sum((1 if doc_id in relevant_set else 0) / np.log2(i + 2) \n",
    "              for i, doc_id in enumerate(retrieved_k))\n",
    "    \n",
    "    # IDCG (ideal DCG)\n",
    "    ideal_k = min(k, len(relevant_ids))\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(ideal_k))\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "# ==================== RESPONSE QUALITY METRICS ====================\n",
    "\n",
    "def calculate_bleu(generated: str, reference: str) -> float:\n",
    "    \"\"\"BLEU Score: N-gram overlap\"\"\"\n",
    "    reference_tokens = reference.split()\n",
    "    generated_tokens = generated.split()\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    return sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothing)\n",
    "\n",
    "def calculate_rouge(generated: str, reference: str) -> Dict[str, float]:\n",
    "    \"\"\"ROUGE Scores: ROUGE-1, ROUGE-2, ROUGE-L\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "    scores = scorer.score(reference, generated)\n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    }\n",
    "\n",
    "def calculate_semantic_similarity(generated: str, reference: str, model) -> float:\n",
    "    \"\"\"Semantic Similarity using PhoBERT embeddings\"\"\"\n",
    "    emb1 = model.encode([generated])\n",
    "    emb2 = model.encode([reference])\n",
    "    return float(cosine_similarity(emb1, emb2)[0][0])\n",
    "\n",
    "# ==================== MEDICAL ACCURACY METRICS ====================\n",
    "\n",
    "def extract_medical_entities(text: str) -> set:\n",
    "    \"\"\"Extract medical entities (simple keyword-based, c√≥ th·ªÉ d√πng NER model)\"\"\"\n",
    "    # Danh s√°ch t·ª´ kh√≥a y t·∫ø ph·ªï bi·∫øn (m·ªü r·ªông theo nhu c·∫ßu)\n",
    "    medical_keywords = [\n",
    "        's·ªët', 'ƒëau', 'vi√™m', 'nhi·ªÖm', 'b·ªánh', 'thu·ªëc', 'paracetamol', 'aspirin',\n",
    "        'xu·∫•t huy·∫øt', 'ti·ªÉu c·∫ßu', 'gan', 'ph·ªïi', 'tim', 'v·∫Øc-xin', 'ƒëi·ªÅu tr·ªã'\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    return {kw for kw in medical_keywords if kw in text_lower}\n",
    "\n",
    "def calculate_entity_accuracy(generated: str, reference: str) -> float:\n",
    "    \"\"\"Medical Entity Accuracy: T·ª∑ l·ªá th·ª±c th·ªÉ y t·∫ø ƒë√∫ng\"\"\"\n",
    "    gen_entities = extract_medical_entities(generated)\n",
    "    ref_entities = extract_medical_entities(reference)\n",
    "    \n",
    "    if len(ref_entities) == 0:\n",
    "        return 1.0 if len(gen_entities) == 0 else 0.0\n",
    "    \n",
    "    correct = len(gen_entities & ref_entities)\n",
    "    return correct / len(ref_entities)\n",
    "\n",
    "print(\"‚úÖ Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_evaluation"
   },
   "source": [
    "## üöÄ 8. Run Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_eval_code"
   },
   "outputs": [],
   "source": [
    "def evaluate_rag_system(test_df: pd.DataFrame, rag_system, phobert_model, k_values=[1, 3, 5]):\n",
    "    \"\"\"\n",
    "    ƒê√°nh gi√° to√†n di·ªán h·ªá th·ªëng RAG\n",
    "    \n",
    "    Returns:\n",
    "        results_df: DataFrame v·ªõi k·∫øt qu·∫£ chi ti·∫øt t·ª´ng c√¢u h·ªèi\n",
    "        metrics_summary: Dict v·ªõi metrics t·ªïng h·ª£p\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"üîç Evaluating {len(test_df)} questions...\\n\")\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating\"):\n",
    "        question = row['question']\n",
    "        expected_answer = row['expected_answer']\n",
    "        relevant_doc_ids = row['relevant_doc_ids']\n",
    "        \n",
    "        # Measure response time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. RETRIEVAL\n",
    "        search_results = rag_system.search(question, top_k=max(k_values))\n",
    "        retrieved_ids = [r['id'] for r in search_results]\n",
    "        \n",
    "        # 2. GENERATION\n",
    "        generated_answer = rag_system.generate_answer(question, search_results)\n",
    "        \n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        # 3. CALCULATE METRICS\n",
    "        result = {\n",
    "            'question': question,\n",
    "            'expected_answer': expected_answer,\n",
    "            'generated_answer': generated_answer,\n",
    "            'retrieved_ids': retrieved_ids,\n",
    "            'relevant_ids': relevant_doc_ids,\n",
    "            'response_time': response_time,\n",
    "        }\n",
    "        \n",
    "        # Retrieval metrics\n",
    "        for k in k_values:\n",
    "            result[f'precision@{k}'] = calculate_precision_at_k(retrieved_ids, relevant_doc_ids, k)\n",
    "            result[f'recall@{k}'] = calculate_recall_at_k(retrieved_ids, relevant_doc_ids, k)\n",
    "            result[f'ndcg@{k}'] = calculate_ndcg_at_k(retrieved_ids, relevant_doc_ids, k)\n",
    "        \n",
    "        result['mrr'] = calculate_mrr(retrieved_ids, relevant_doc_ids)\n",
    "        \n",
    "        # Response quality metrics\n",
    "        result['bleu'] = calculate_bleu(generated_answer, expected_answer)\n",
    "        rouge_scores = calculate_rouge(generated_answer, expected_answer)\n",
    "        result.update(rouge_scores)\n",
    "        result['semantic_similarity'] = calculate_semantic_similarity(\n",
    "            generated_answer, expected_answer, phobert_model\n",
    "        )\n",
    "        \n",
    "        # Medical accuracy\n",
    "        result['entity_accuracy'] = calculate_entity_accuracy(generated_answer, expected_answer)\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    metrics_summary = {}\n",
    "    \n",
    "    # Retrieval metrics\n",
    "    for k in k_values:\n",
    "        metrics_summary[f'Precision@{k}'] = results_df[f'precision@{k}'].mean()\n",
    "        metrics_summary[f'Recall@{k}'] = results_df[f'recall@{k}'].mean()\n",
    "        metrics_summary[f'NDCG@{k}'] = results_df[f'ndcg@{k}'].mean()\n",
    "    \n",
    "    metrics_summary['MRR'] = results_df['mrr'].mean()\n",
    "    \n",
    "    # Response quality\n",
    "    metrics_summary['BLEU'] = results_df['bleu'].mean()\n",
    "    metrics_summary['ROUGE-1'] = results_df['rouge1'].mean()\n",
    "    metrics_summary['ROUGE-2'] = results_df['rouge2'].mean()\n",
    "    metrics_summary['ROUGE-L'] = results_df['rougeL'].mean()\n",
    "    metrics_summary['Semantic Similarity'] = results_df['semantic_similarity'].mean()\n",
    "    \n",
    "    # Medical accuracy\n",
    "    metrics_summary['Entity Accuracy'] = results_df['entity_accuracy'].mean()\n",
    "    \n",
    "    # Performance\n",
    "    metrics_summary['Avg Response Time (s)'] = results_df['response_time'].mean()\n",
    "    \n",
    "    return results_df, metrics_summary\n",
    "\n",
    "# RUN EVALUATION\n",
    "results_df, metrics_summary = evaluate_rag_system(test_df, rag_system, phobert)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in metrics_summary.items():\n",
    "    print(f\"{metric:.<40} {value:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize"
   },
   "source": [
    "## üìà 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz_code"
   },
   "outputs": [],
   "source": [
    "# ==================== METRICS COMPARISON ====================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Retrieval Metrics\n",
    "retrieval_metrics = {\n",
    "    'Precision@1': metrics_summary['Precision@1'],\n",
    "    'Precision@3': metrics_summary['Precision@3'],\n",
    "    'Recall@1': metrics_summary['Recall@1'],\n",
    "    'Recall@3': metrics_summary['Recall@3'],\n",
    "    'MRR': metrics_summary['MRR'],\n",
    "    'NDCG@3': metrics_summary['NDCG@3']\n",
    "}\n",
    "axes[0, 0].bar(retrieval_metrics.keys(), retrieval_metrics.values(), color='skyblue')\n",
    "axes[0, 0].set_title('Retrieval Metrics', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Score')\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 2. Response Quality Metrics\n",
    "quality_metrics = {\n",
    "    'BLEU': metrics_summary['BLEU'],\n",
    "    'ROUGE-1': metrics_summary['ROUGE-1'],\n",
    "    'ROUGE-2': metrics_summary['ROUGE-2'],\n",
    "    'ROUGE-L': metrics_summary['ROUGE-L'],\n",
    "    'Semantic Sim': metrics_summary['Semantic Similarity']\n",
    "}\n",
    "axes[0, 1].bar(quality_metrics.keys(), quality_metrics.values(), color='lightcoral')\n",
    "axes[0, 1].set_title('Response Quality Metrics', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[0, 1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 3. Per-Question Performance\n",
    "results_df['question_short'] = results_df['question'].str[:30] + '...'\n",
    "x_pos = np.arange(len(results_df))\n",
    "axes[1, 0].bar(x_pos, results_df['semantic_similarity'], alpha=0.7, label='Semantic Sim', color='green')\n",
    "axes[1, 0].bar(x_pos, results_df['bleu'], alpha=0.7, label='BLEU', color='orange')\n",
    "axes[1, 0].set_title('Per-Question Performance', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Question Index')\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Response Time Distribution\n",
    "axes[1, 1].hist(results_df['response_time'], bins=10, color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].axvline(results_df['response_time'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {results_df[\"response_time\"].mean():.2f}s')\n",
    "axes[1, 1].set_title('Response Time Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Response Time (seconds)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================== DETAILED RESULTS TABLE ====================\n",
    "print(\"\\nüìã DETAILED RESULTS (First 3 Questions)\\n\")\n",
    "display_cols = ['question', 'precision@3', 'recall@3', 'bleu', 'semantic_similarity', 'entity_accuracy']\n",
    "print(results_df[display_cols].head(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export"
   },
   "source": [
    "## üíæ 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_code"
   },
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "results_df.to_csv('evaluation_results.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Export summary to JSON\n",
    "with open('metrics_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ Results exported:\")\n",
    "print(\"   - evaluation_results.csv\")\n",
    "print(\"   - metrics_summary.json\")\n",
    "\n",
    "# Download files (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('evaluation_results.csv')\n",
    "    files.download('metrics_summary.json')\n",
    "except:\n",
    "    print(\"\\n(Not in Colab - files saved locally)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interpretation"
   },
   "source": [
    "## üìñ 11. Interpretation Guide\n",
    "\n",
    "### üéØ C√°ch ƒê√°nh Gi√° M√¥ H√¨nh T·ªët Hay Kh√¥ng\n",
    "\n",
    "#### **A. Retrieval Metrics (Kh·∫£ nƒÉng t√¨m ki·∫øm)**\n",
    "\n",
    "| Metric | T·ªët | Trung B√¨nh | Y·∫øu | √ù Nghƒ©a |\n",
    "|--------|-----|------------|-----|----------|\n",
    "| **Precision@3** | ‚â• 0.8 | 0.5-0.8 | < 0.5 | T·ª∑ l·ªá t√†i li·ªáu ƒë√∫ng trong top 3 |\n",
    "| **Recall@3** | ‚â• 0.9 | 0.6-0.9 | < 0.6 | T√¨m ƒë∆∞·ª£c bao nhi√™u % t√†i li·ªáu li√™n quan |\n",
    "| **MRR** | ‚â• 0.8 | 0.5-0.8 | < 0.5 | T√†i li·ªáu ƒë√∫ng c√≥ ·ªü v·ªã tr√≠ cao kh√¥ng |\n",
    "| **NDCG@3** | ‚â• 0.85 | 0.6-0.85 | < 0.6 | Ch·∫•t l∆∞·ª£ng ranking t·ªïng th·ªÉ |\n",
    "\n",
    "**N·∫øu Retrieval k√©m:**\n",
    "- ‚úÖ C·∫£i thi·ªán embedding (fine-tune PhoBERT)\n",
    "- ‚úÖ TƒÉng tr·ªçng s·ªë BM25 (keyword matching)\n",
    "- ‚úÖ Th√™m query expansion\n",
    "- ‚úÖ Reranking v·ªõi Cross-Encoder\n",
    "\n",
    "---\n",
    "\n",
    "#### **B. Response Quality Metrics (Ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi)**\n",
    "\n",
    "| Metric | T·ªët | Trung B√¨nh | Y·∫øu | √ù Nghƒ©a |\n",
    "|--------|-----|------------|-----|----------|\n",
    "| **BLEU** | ‚â• 0.4 | 0.2-0.4 | < 0.2 | ƒê·ªô gi·ªëng v·ªÅ t·ª´ ng·ªØ |\n",
    "| **ROUGE-L** | ‚â• 0.5 | 0.3-0.5 | < 0.3 | ƒê·ªô ch·ªìng l·∫•p c√¢u d√†i |\n",
    "| **Semantic Similarity** | ‚â• 0.75 | 0.6-0.75 | < 0.6 | ƒê·ªô gi·ªëng v·ªÅ √Ω nghƒ©a (quan tr·ªçng nh·∫•t!) |\n",
    "| **Entity Accuracy** | ‚â• 0.85 | 0.7-0.85 | < 0.7 | T·ª∑ l·ªá thu·∫≠t ng·ªØ y t·∫ø ƒë√∫ng |\n",
    "\n",
    "**N·∫øu Response Quality k√©m:**\n",
    "- ‚úÖ C·∫£i thi·ªán prompt cho GPT\n",
    "- ‚úÖ TƒÉng s·ªë l∆∞·ª£ng context (top K results)\n",
    "- ‚úÖ Fine-tune GPT v·ªõi medical data\n",
    "- ‚úÖ Th√™m fact-checking layer\n",
    "\n",
    "---\n",
    "\n",
    "#### **C. System Performance**\n",
    "\n",
    "| Metric | T·ªët | Ch·∫•p Nh·∫≠n ƒê∆∞·ª£c | Y·∫øu |\n",
    "|--------|-----|----------------|-----|\n",
    "| **Response Time** | < 2s | 2-5s | > 5s |\n",
    "| **Token Usage** | < 500 tokens/query | 500-1000 | > 1000 |\n",
    "\n",
    "---\n",
    "\n",
    "### üîç V√≠ D·ª• Ph√¢n T√≠ch\n",
    "\n",
    "**K·ªãch b·∫£n 1: Retrieval t·ªët nh∆∞ng Response k√©m**\n",
    "```\n",
    "Precision@3: 0.85 ‚úÖ\n",
    "Semantic Similarity: 0.45 ‚ùå\n",
    "```\n",
    "‚Üí **Nguy√™n nh√¢n:** GPT kh√¥ng t·ªïng h·ª£p th√¥ng tin t·ªët  \n",
    "‚Üí **Gi·∫£i ph√°p:** C·∫£i thi·ªán prompt, th√™m examples\n",
    "\n",
    "**K·ªãch b·∫£n 2: Retrieval k√©m nh∆∞ng Response t·ªët**\n",
    "```\n",
    "Precision@3: 0.40 ‚ùå\n",
    "Semantic Similarity: 0.80 ‚úÖ\n",
    "```\n",
    "‚Üí **Nguy√™n nh√¢n:** GPT t·ª± suy lu·∫≠n (hallucination!)  \n",
    "‚Üí **Gi·∫£i ph√°p:** C·∫£i thi·ªán retrieval, gi·∫£m temperature GPT\n",
    "\n",
    "**K·ªãch b·∫£n 3: C·∫£ 2 ƒë·ªÅu t·ªët**\n",
    "```\n",
    "Precision@3: 0.90 ‚úÖ\n",
    "Semantic Similarity: 0.85 ‚úÖ\n",
    "```\n",
    "‚Üí **M√¥ h√¨nh ƒë·∫°t chu·∫©n production!** üéâ\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Checklist ƒê√°nh Gi√° Cu·ªëi C√πng\n",
    "\n",
    "- [ ] **Retrieval Precision@3 ‚â• 0.7**\n",
    "- [ ] **Semantic Similarity ‚â• 0.7**\n",
    "- [ ] **Entity Accuracy ‚â• 0.8**\n",
    "- [ ] **Response Time < 3s**\n",
    "- [ ] **Kh√¥ng c√≥ hallucination nghi√™m tr·ªçng**\n",
    "\n",
    "N·∫øu ƒë·∫°t **4/5 ti√™u ch√≠** ‚Üí M√¥ h√¨nh **T·ªêT**, c√≥ th·ªÉ deploy!\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
